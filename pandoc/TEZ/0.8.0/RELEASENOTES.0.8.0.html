<!---
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
-->
<h1 id="apache-tez-0.8.0-release-notes">Apache Tez 0.8.0 Release Notes</h1>
<p>These release notes cover new developer and user-facing incompatibilities, features, and major improvements.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2719">TEZ-2719</a> | <em>Major</em> | <strong>Consider reducing logs in unordered fetcher with shared-fetch option</strong></li>
</ul>
<p>For large broadcast, this can be a problem e.g In one of the jobs (query_17 @ 10 TB scale), Map 7 generates around 1.1 GB of data which is given to 330 tasks in downstream Map 1.</p>
<p>Map 1 uses all slots in cluster (~ 224 per wave). Until data is downloaded, shared fetch would end up re-queuing fetches. As a part of it, it would end up printing 3 logs per attempt. E.g</p>
<p>{noformat} 2015-08-14 02:09:11,761 INFO [Fetcher [Map_7] #0] shuffle.Fetcher: Requeuing machine1:13562 downloads because we didn't get a lock 2015-08-14 02:09:11,761 INFO [Fetcher [Map_7] #0] shuffle.Fetcher: Shared fetch failed to return 1 inputs on this try 2015-08-14 02:09:11,761 INFO [ShuffleRunner [Map_7]] impl.ShuffleManager: Scheduling fetch for inputHost: machine1:13562 2015-08-14 02:09:11,761 INFO [ShuffleRunner [Map_7]] impl.ShuffleManager: Created Fetcher for host: machine1 with inputs: [InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=0], attemptNumber=0, pathComponent=attempt_1439264591968_0058_1_04_000000_0_10029, fetchTypeInfo=FINAL_MERGE_ENABLED, spillEventId=-1]] {noformat}</p>
<p>Based on disk / network, it might take time for fetcher to finish downloading and release the lock. Since there was only one task in Map-1, it ended up in a sort of tight loop generating relatively larger logs.</p>
<p>Looks like 260-290 MB task log files are created in this case per attempt. That would be around 2.3 GB to 3 GB (depending on number of slots waiting) in machine with 8-10 slots.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2701">TEZ-2701</a> | <em>Major</em> | <strong>Add time at which container was allocated to attempt</strong></li>
</ul>
<p>We have startTime but not allocation time and launch start time. So we don't know how long it took to allocate and how long it took to launch.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2684">TEZ-2684</a> | <em>Major</em> | <strong>ShuffleVertexManager.parsePartitionStats throws IllegalStateException: Stats should be initialized</strong></li>
</ul>
<p>When I run hive qfile test (attached) using TestMiniTezCliDriver. My WIP patch is also attached for problem reproduction purpose, as well as hive.log.</p>
<p>Here's the explain and backtrace I got from qfile output: {code} EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.<code>date</code> = '2008-04-08' POSTHOOK: type: QUERY STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 depends on stages: Stage-1</p>
<p>STAGE PLANS: Stage: Stage-1 Tez Edges: Reducer 2 &lt;- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE) Reducer 3 &lt;- Reducer 2 (SIMPLE_EDGE) DagName: wzheng_20150803161620_55c139de-c26c-467f-b592-7d4333053ac6:38 Vertices: Map 1 Map Operator Tree: TableScan alias: srcpart filterExpr: ds is not null (type: boolean) Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: ds (type: string) sort order: + Map-reduce partition columns: ds (type: string) Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE Map 4 Map Operator Tree: TableScan alias: srcpart_date filterExpr: (ds is not null and (date = '2008-04-08')) (type: boolean) Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (ds is not null and (date = '2008-04-08')) (type: boolean) Statistics: Num rows: 1 Data size: 21 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: ds (type: string) sort order: + Map-reduce partition columns: ds (type: string) Statistics: Num rows: 1 Data size: 21 Basic stats: COMPLETE Column stats: NONE Select Operator expressions: ds (type: string) outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 21 Basic stats: COMPLETE Column stats: NONE Group By Operator keys: _col0 (type: string) mode: hash outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 21 Basic stats: COMPLETE Column stats: NONE Dynamic Partitioning Event Operator Target Input: srcpart Partition key expr: ds Statistics: Num rows: 1 Data size: 21 Basic stats: COMPLETE Column stats: NONE Target column: ds Target Vertex: Map 1 Reducer 2 Reduce Operator Tree: Merge Join Operator condition map: Inner Join 0 to 1 keys: 0 ds (type: string) 1 ds (type: string) Statistics: Num rows: 2200 Data size: 23372 Basic stats: COMPLETE Column stats: NONE Group By Operator aggregations: count() mode: hash outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator sort order: Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE value expressions: _col0 (type: bigint) Reducer 3 Reduce Operator Tree: Group By Operator aggregations: count(VALUE._col0) mode: mergepartial outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE File Output Operator compressed: false Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</p>
<p>Stage: Stage-0 Fetch Operator limit: -1 Processor Tree: ListSink</p>
<p>PREHOOK: query: select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.<code>date</code> = '2008-04-08' PREHOOK: type: QUERY PREHOOK: Input: default@srcpart PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12 PREHOOK: Input: default@srcpart_date PREHOOK: Output: file:/Users/wzheng/bf/hive/itests/qtest/target/tmp/localscratchdir/93b335b5-3ced-4f4d-abdd-2fd5defd11e4/hive_2015-08-03_16-16-21_046_5066458626645110592-1/-mr-10001 Status: Failed Vertex failed, vertexName=Reducer 2, vertexId=vertex_1438643776809_0001_8_02, diagnostics=[Vertex vertex_1438643776809_0001_8_02 [Reducer 2] killed/failed due to:AM_USERCODE_FAILURE, Exception in VertexManager, vertex:vertex_1438643776809_0001_8_02 [Reducer 2], java.lang.IllegalStateException: Stats should be initialized at com.google.common.base.Preconditions.checkState(Preconditions.java:149) at org.apache.tez.dag.library.vertexmanager.ShuffleVertexManager.parsePartitionStats(ShuffleVertexManager.java:535) at org.apache.tez.dag.library.vertexmanager.ShuffleVertexManager.onVertexManagerEventReceived(ShuffleVertexManager.java:575) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEventReceived.invoke(VertexManager.java:602) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent$1.run(VertexManager.java:643) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent$1.run(VertexManager.java:638) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent.call(VertexManager.java:638) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent.call(VertexManager.java:627) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) ] Vertex killed, vertexName=Reducer 3, vertexId=vertex_1438643776809_0001_8_03, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1438643776809_0001_8_03 [Reducer 3] killed/failed due to:null] Vertex killed, vertexName=Map 1, vertexId=vertex_1438643776809_0001_8_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1438643776809_0001_8_01 [Map 1] killed/failed due to:null] DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:2 FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_1438643776809_0001_8_02, diagnostics=[Vertex vertex_1438643776809_0001_8_02 [Reducer 2] killed/failed due to:AM_USERCODE_FAILURE, Exception in VertexManager, vertex:vertex_1438643776809_0001_8_02 [Reducer 2], java.lang.IllegalStateException: Stats should be initialized at com.google.common.base.Preconditions.checkState(Preconditions.java:149) at org.apache.tez.dag.library.vertexmanager.ShuffleVertexManager.parsePartitionStats(ShuffleVertexManager.java:535) at org.apache.tez.dag.library.vertexmanager.ShuffleVertexManager.onVertexManagerEventReceived(ShuffleVertexManager.java:575) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEventReceived.invoke(VertexManager.java:602) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent$1.run(VertexManager.java:643) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent$1.run(VertexManager.java:638) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent.call(VertexManager.java:638) at org.apache.tez.dag.app.dag.impl.VertexManager$VertexManagerEvent.call(VertexManager.java:627) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) ]Vertex killed, vertexName=Reducer 3, vertexId=vertex_1438643776809_0001_8_03, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1438643776809_0001_8_03 [Reducer 3] killed/failed due to:null]Vertex killed, vertexName=Map 1, vertexId=vertex_1438643776809_0001_8_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1438643776809_0001_8_01 [Map 1] killed/failed due to:null]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:2 {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2683">TEZ-2683</a> | <em>Major</em> | <strong>TestHttpConnection::testAsyncHttpConnectionInterrupt fails in certain environments</strong></li>
</ul>
<p>2015-08-03 11:36:10,064 ERROR [pool-4-thread-1] netty.TezBodyDeferringAsyncHandler (TezBodyDeferringAsyncHandler.java:onThrowable(84)) - Error in asyncHandler java.net.ConnectException: Permission denied to http://10.255.255.255:10221/ at com.ning.http.client.providers.netty.NettyConnectListener.operationComplete(NettyConnectListener.java:104) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:145) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.doConnect(NettyAsyncHttpProvider.java:1139) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.execute(NettyAsyncHttpProvider.java:940) at com.ning.http.client.AsyncHttpClient.executeRequest(AsyncHttpClient.java:499) at org.apache.tez.http.async.netty.AsyncHttpConnection.connect(AsyncHttpConnection.java:154) at org.apache.tez.http.async.netty.AsyncHttpConnection$$EnhancerByMockitoWithCGLIB$$1efd06aa.CGLIB$connect$3(<generated>) at org.apache.tez.http.async.netty.AsyncHttpConnection$$EnhancerByMockitoWithCGLIB$$1efd06aa$$FastClassByMockitoWithCGLIB$$d440a4cd.invoke(<generated>) at org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216) at org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10) at org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22) at org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27) at org.mockito.internal.invocation.InvocationImpl.callRealMethod(InvocationImpl.java:112) at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36) at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:93) at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:38) at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:61) at org.apache.tez.http.async.netty.AsyncHttpConnection$$EnhancerByMockitoWithCGLIB$$1efd06aa.connect(<generated>) at org.apache.tez.http.TestHttpConnection$Worker.call(TestHttpConnection.java:186) at org.apache.tez.http.TestHttpConnection$Worker.call(TestHttpConnection.java:171) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Caused by: java.net.SocketException: Permission denied at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:465) at sun.nio.ch.Net.connect(Net.java:457) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:670)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2663">TEZ-2663</a> | <em>Major</em> | <strong>SessionNotRunning exceptions are wrapped in a ServiceException from a dying AM</strong></li>
</ul>
<p>The scenario in TEZ-2548 throws a SessionNotRunning from the AM right now, which gets wrapped in a ServiceException by the protobuf layer, so that the exception thrown by TezClient cannot be caught and handled cleanly.</p>
<p>{code} 2015-07-30 01:32:50,997 ERROR [HiveServer2-Background-Pool: Thread-494()]: exec.Task (TezTask.java:execute(191)) - Failed to execute tez graph. org.apache.tez.dag.api.TezException: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.tez.dag.api.SessionNotRunning): AM unable to accept new DAG submissions. In the process of shutting down at org.apache.tez.dag.app.DAGAppMaster.submitDAGToAppMaster(DAGAppMaster.java:1265) at org.apache.tez.dag.api.client.DAGClientHandler.submitDAG(DAGClientHandler.java:120) at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPBServerImpl.submitDAG(DAGClientAMProtocolBlockingPBServerImpl.java:161) at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolRPC$DAGClientAMProtocol$2.callBlockingMethod(DAGClientAMProtocolRPC.java:7471) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045) {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2650">TEZ-2650</a> | <em>Major</em> | <strong>Timing details on Vertex state changes</strong></li>
</ul>
<p>E.g. how long did it wait to get initialized - tasks or edges. Did it wait for a long time for a predecessor vertex to initialize.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2647">TEZ-2647</a> | <em>Major</em> | <strong>Add input causality dependency for attempts</strong></li>
</ul>
<p>Attempts can have input dependencies on the producer task attempts that produced the data being consumed by the attempt. DataMovement events capture this dependency. In the interest of space, we need to be able to capture the dependency that matters - the one that provided the last data for the input to complete. For starters, we could 1) have the system track the last data movement event that was sent to an attempt 2) then have the inputs be able to report the last relevant data movement event</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2646">TEZ-2646</a> | <em>Major</em> | <strong>Add scheduling casual dependency for attempts</strong></li>
</ul>
<p>When a task gets scheduled then we dont know what caused it. Some possibilities are 1) initial scheduling by the vertex manager - causality determined by VM. E.g. dynamic partition pruning VM in Hive can point causality to the attempt that sent it the stats needed to complete the partition pruning logic. 2) re-scheduling due to own previous version failure - causality points to the previous version that just failed 3) re-scheduling because read error reported by consumer - causality points to the consumer attempt that reported the error and caused the scheduling.</p>
<p>This causality relationship can be used to stitch together scheduling dependencies in the execution timeline of the DAG.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2645">TEZ-2645</a> | <em>Major</em> | <strong>Provide standard analyzers for job analysis</strong></li>
</ul>
<p>TEZ-2076 provided a way in which job history data can be parsed/normalized and represented in-memory. Based on this, standard set of analyzers (e.g ContainerReuseAnalyzer, LocalityAnalyzer, SlowNodeAnalyzer etc) can provided in tez-tools for job analysis. Results can be stored in CSVResult which could be rendered differently based on UI requirements.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2636">TEZ-2636</a> | <em>Major</em> | <strong>MRInput and MultiMRInput should work for cases when there are 0 physical inputs</strong></li>
</ul>
<p>It's possible that an Input is setup without any actual data. This is especially valid when a task is processing multiple MRInputs. One side has data, but the other does not. In such cases - we currently end up generating an error.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2635">TEZ-2635</a> | <em>Major</em> | <strong>Limit number of attempts being downloaded in unordered fetch</strong></li>
</ul>
<p>{noformat} 2015-07-22 23:39:14,221 WARN [Fetcher [Map_3] #4] shuffle.Fetcher: Fetch Failure from host while connecting: machine123, attempt: InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=12], attemptNumber=0, pathComponent=attempt_1437098194051_0178_2_02_000012_0_10003_0, fetchTypeInfo=INCREMENTAL_UPDATE, spillEventId=0] Informing ShuffleManager: java.io.IOException: Server returned HTTP response code: 400 for URL: http://machine123:13562/mapOutput?job=job_1437098194051_0178&amp;reduce=279&amp;map=attempt_1437098194051_0178_2_02_000012_0_10003_0,attempt_1437098194051_0178_2_02_000012_0_10003_1,attempt_1437098194051_0178_2_02_000012_0_10003_2,attempt_1437098194051_0178_2_02_000012_0_10003_3,attempt_1437098194051_0178_2_02_000031_0_10006_0,attempt_1437098194051_0178_2_02_000031_0_10006_1,attempt_1437098194051_0178_2_02_000031_0_10006_2,attempt_1437098194051_0178_2_02_000031_0_10006_3,attempt_1437098194051_0178_2_02_000031_0_10006_4,attempt_1437098194051_0178_2_02_000050_0_10009_0,attempt_1437098194051_0178_2_02_000050_0_10009_1,attempt_1437098194051_0178_2_02_000050_0_10009_2,attempt_1437098194051_0178_2_02_000050_0_10009_3,attempt_1437098194051_0178_2_02_000069_0_10012_0,attempt_1437098194051_0178_2_02_000088_0_10033_0,attempt_1437098194051_0178_2_02_000107_0_10033_0,attempt_1437098194051_0178_2_02_000126_0_10006_0,attempt_1437098194051_0178_2_02_000069_0_10012_1,attempt_1437098194051_0178_2_02_000088_0_10033_1,attempt_1437098194051_0178_2_02_000145_0_10006_0,attempt_1437098194051_0178_2_02_000107_0_10033_1,attempt_1437098194051_0178_2_02_000126_0_10006_1,attempt_1437098194051_0178_2_02_000069_0_10012_2,attempt_1437098194051_0178_2_02_000069_0_10012_3,attempt_1437098194051_0178_2_02_000145_0_10006_1,attempt_1437098194051_0178_2_02_000088_0_10033_2,attempt_1437098194051_0178_2_02_000107_0_10033_2,attempt_1437098194051_0178_2_02_000126_0_10006_2,attempt_1437098194051_0178_2_02_000164_0_10030_0,attempt_1437098194051_0178_2_02_000183_0_10006_0,attempt_1437098194051_0178_2_02_000107_0_10033_3,attempt_1437098194051_0178_2_02_000145_0_10006_2,attempt_1437098194051_0178_2_02_000088_0_10033_3,attempt_1437098194051_0178_2_02_000088_0_10033_4,attempt_1437098194051_0178_2_02_000202_0_10015_0,attempt_1437098194051_0178_2_02_000145_0_10006_3,attempt_1437098194051_0178_2_02_000126_0_10006_3,attempt_1437098194051_0178_2_02_000126_0_10006_4,attempt_1437098194051_0178_2_02_000164_0_10030_1,attempt_1437098194051_0178_2_02_000183_0_10006_1,attempt_1437098194051_0178_2_02_000202_0_10015_1,attempt_1437098194051_0178_2_02_000183_0_10006_2,attempt_1437098194051_0178_2_02_000164_0_10030_2,attempt_1437098194051_0178_2_02_000164_0_10030_3,attempt_1437098194051_0178_2_02_000183_0_10006_3,attempt_1437098194051_0178_2_02_000202_0_10015_2,attempt_1437098194051_0178_2_02_000202_0_10015_3,attempt_1437098194051_0178_2_02_000133_0_10036_0,attempt_1437098194051_0178_2_02_000096_0_10012_0,attempt_1437098194051_0178_2_02_000114_0_10009_0,attempt_1437098194051_0178_2_02_000095_0_10009_0,attempt_1437098194051_0178_2_02_000153_0_10041_0,attempt_1437098194051_0178_2_02_000143_0_10036_0,attempt_1437098194051_0178_2_02_000190_0_10015_0,attempt_1437098194051_0178_2_02_000181_0_10042_0,attempt_1437098194051_0178_2_02_000133_0_10036_1,attempt_1437098194051_0178_2_02_000143_0_10036_1,attempt_1437098194051_0178_2_02_000153_0_10041_1,attempt_1437098194051_0178_2_02_000190_0_10015_1,attempt_1437098194051_0178_2_02_000209_0_10018_0,attempt_1437098194051_0178_2_02_000095_0_10009_1,attempt_1437098194051_0178_2_02_000114_0_10009_1,attempt_1437098194051_0178_2_02_000096_0_10012_1,attempt_1437098194051_0178_2_02_000181_0_10042_1,attempt_1437098194051_0178_2_02_000133_0_10036_2,attempt_1437098194051_0178_2_02_000153_0_10041_2,attempt_1437098194051_0178_2_02_000143_0_10036_2,attempt_1437098194051_0178_2_02_000114_0_10009_2,attempt_1437098194051_0178_2_02_000190_0_10015_2,attempt_1437098194051_0178_2_02_000133_0_10036_3,attempt_1437098194051_0178_2_02_000095_0_10009_2,attempt_1437098194051_0178_2_02_000096_0_10012_2,attempt_1437098194051_0178_2_02_000209_0_10018_1,attempt_1437098194051_0178_2_02_000181_0_10042_2,attempt_1437098194051_0178_2_02_000153_0_10041_3,attempt_1437098194051_0178_2_02_000095_0_10009_3,attempt_1437098194051_0178_2_02_000096_0_10012_3,attempt_1437098194051_0178_2_02_000114_0_10009_3,attempt_1437098194051_0178_2_02_000190_0_10015_3,attempt_1437098194051_0178_2_02_000143_0_10036_3,attempt_1437098194051_0178_2_02_000190_0_10015_4,attempt_1437098194051_0178_2_02_000143_0_10036_4,attempt_1437098194051_0178_2_02_000181_0_10042_3,attempt_1437098194051_0178_2_02_000153_0_10041_4,attempt_1437098194051_0178_2_02_000181_0_10042_4,attempt_1437098194051_0178_2_02_000209_0_10018_2,attempt_1437098194051_0178_2_02_000209_0_10018_3&amp;keepAlive=true at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1839) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440) at org.apache.tez.http.HttpConnection.getInputStream(HttpConnection.java:248) at org.apache.tez.runtime.library.common.shuffle.Fetcher.setupConnection(Fetcher.java:441) at org.apache.tez.runtime.library.common.shuffle.Fetcher.doHttpFetch(Fetcher.java:470) at org.apache.tez.runtime.library.common.shuffle.Fetcher.doHttpFetch(Fetcher.java:403) at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:199) at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:71) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) {noformat}</p>
<p>tez.runtime.shuffle.fetch.max.task.output.at.once is provided only for ordered fetch, which defaults to 20. But for unordered case, this is not honored.</p>
<p>[~gopalv] got this issue when executing &quot;select p.p_partkey, li.l_suppkey from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey where li.l_linenumber = 1 and li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR') limit 2&quot; @ 10 TB scale</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2633">TEZ-2633</a> | <em>Major</em> | <strong>Allow VertexManagerPlugins to receive and report based on attempts instead of tasks</strong></li>
</ul>
<p>If the same event is sent from an attempt and its retry then there is no way to differentiate between them.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2632">TEZ-2632</a> | <em>Major</em> | <strong>A -Paws and -Pazure build profiles for hadoop-{aws,azure} inclusion</strong></li>
</ul>
<p>A number of S3a workloads which work with MRv2 (after HADOOP-10400) is failing to work with Tez.</p>
<p>That compatibility issue can be bypassed with a trivial build profile switch (same for azure after HADOOP-9629)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2630">TEZ-2630</a> | <em>Critical</em> | <strong>TezChild receives IP address instead of FQDN</strong></li>
</ul>
<p>I am running a yarn cluster on AWS. The slave nodes (NMs) are all configured to listen on private DNS. For example, a sample node manager listens on ip-10-16-141-168.ec2.internal:8042.</p>
<p>When I'm trying to run a Tez job (even simple ones like select count(*) from nation) - they fail because child tasks are unable to connect to the AM. The issue is they are trying to connect to the IP instead of the private DNS. Here's a sample log line (couple of them added by me for debugging):</p>
<p>{code} 2015-07-21 17:08:21,919 INFO [main] task.TezChild: TezChild starting 2015-07-21 17:08:22,310 INFO [main] task.TezChild: Using socket factory class: org.apache.hadoop.net.StandardSocketFactory 2015-07-21 17:08:22,336 INFO [main] task.TezChild: PID, containerIdentifier: 3699, container_1437498369268_0001_01_000002 2015-07-21 17:08:22,418 INFO [main] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS 2015-07-21 17:08:23,025 INFO [main] task.TezChild: Got host:port: 10.16.141.168:37949 2015-07-21 17:08:23,035 INFO [main] task.TezChild: address variables: 10.16.141.168:37949 2015-07-21 17:08:23,143 INFO [TezChild] task.ContainerReporter: Attempting to fetch new task 2015-07-21 17:08:24,201 INFO [TezChild] ipc.Client: Retrying connect to server: 10.16.141.168/10.16.141.168:37949. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) 2015-07-21 17:08:25,202 INFO [TezChild] ipc.Client: Retrying connect to server: 10.16.141.168/10.16.141.168:37949. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) 2015-07-21 17:08:26,757 INFO [TezChild] ipc.Client: Retrying connect to server: 10.16.141.168/10.16.141.168:37949. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) 2015-07-21 17:08:27,758 INFO [TezChild] ipc.Client: Retrying connect to server: 10.16.141.168/10.16.141.168:37949. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) {code}</p>
<p>AM is listening at the right address. But TezChild is receiving the IP address instead of the private DNS.</p>
<p>AM logs: {code} 2015-07-21 18:09:27,906 INFO [ServiceThread:org.apache.tez.dag.app.TaskAttemptListenerImpTezDag] app.TaskAttemptListenerImpTezDag: Listening at address: ip-10-234-2-80.ec2.internal:49967 {code}</p>
<p>TezChild logs: {code} 2015-07-21 18:09:35,353 INFO [main] task.TezChild: TezChild starting 2015-07-21 18:09:35,379 INFO [main] task.TezChild: Args: 10.234.2.80,49967,container_1437501941642_0001_01_000002,application_1437501941642_0001,1 2015-07-21 18:09:35,770 INFO [main] task.TezChild: Using socket factory class: org.apache.hadoop.net.StandardSocketFactory 2015-07-21 18:09:35,785 INFO [main] task.TezChild: PID, containerIdentifier: 8670, container_1437501941642_0001_01_000002 2015-07-21 18:09:35,864 INFO [main] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS 2015-07-21 18:09:36,403 INFO [main] task.TezChild: Got host:port: 10.234.2.80:49967 2015-07-21 18:09:36,413 INFO [main] task.TezChild: address variables: 10.234.2.80:49967 {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2627">TEZ-2627</a> | <em>Major</em> | <strong>Support for Tez Job Priorities</strong></li>
</ul>
<p>When a Tez Job is submitted via TezClient, an ApplicationSubmissionContext is created before submitting the job. ApplicationSubmissionContext has a priority field which can be used to provide a priority for the job.</p>
<p>There is an ongoing effort in the Yarn Community to enable application priorities(https://issues.apache.org/jira/browse/YARN-1963).</p>
<p>https://issues.apache.org/jira/browse/YARN-2003 implements the necessary changes in RM and Capacity Scheduler.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2623">TEZ-2623</a> | <em>Major</em> | <strong>Fix module dependencies related to hadoop-auth</strong></li>
</ul>
<p>Tez doesn't compile when the {{.m2}} directory is empty. It needs to depend on {{hadoop-auth}} as well.</p>
<p>{code} [ERROR] /Users/rjain/workspace/tez/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/shuffle/HttpConnection.java:[402,51] cannot access org.apache.hadoop.security.authentication.client.ConnectionConfigurator class file for org.apache.hadoop.security.authentication.client.ConnectionConfigurator not found [INFO] 1 error {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2616">TEZ-2616</a> | <em>Major</em> | <strong>Fix build warning by undefined version of maven-findbugs-plugin</strong></li>
</ul>
<p>{quote} [WARNING] Some problems were encountered while building the effective model for org.apache.tez:tez-api:jar:0.8.0-SNAPSHOT [WARNING] 'reporting.plugins.plugin.version' for org.codehaus.mojo:findbugs-maven-plugin is missing. @ org.apache.tez:tez:0.8.0-SNAPSHOT, /home/ubuntu/tezdev/pom.xml, line 1148, column 15 {quote}</p>
<p>This problem is caused since the plugin is used not only in build section, but also reporting section. The usage is same as maven-javadoc-plugin, so similar issue can happen for maven-javadoc-plugin. This JIRA suggests we add new properties findbugs-maven-plugin.version and javadoc-maven-plugin.version for avoiding similar issues.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2613">TEZ-2613</a> | <em>Major</em> | <strong>Fetcher(unordered) using List to store InputAttemptIdentifier can lead to some inefficiency during remove() operation</strong></li>
</ul>
<p>remove() operation on the remaining list can be inefficient.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2610">TEZ-2610</a> | <em>Major</em> | <strong>Swimlane for DAGs that use containers from previous DAGs</strong></li>
</ul>
<p>PROBELM:</p>
<p>Swimlane can not draw for DAGs that uses containers from previous DAGs.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2602">TEZ-2602</a> | <em>Major</em> | <strong>Throwing EOFException when launching MR job</strong></li>
</ul>
<p>{quote} $hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount -Dmapreduce.framework.name=yarn-tez -Dmapr ed.reduce.tasks=15 -Dtez.runtime.sort.threads=1 wc10g tezwc10g5 15/07/07 13:24:30 INFO client.RMProxy: Connecting to ResourceManager at /127.0.0.1:8081<br />
15/07/07 13:24:30 INFO client.AHSProxy: Connecting to Application History server at /0.0.0.0:10200 15/07/07 13:24:30 INFO mapreduce.Job: The url to track the job: http://ip-172-31-4-8.ap-northeast-1.compute.internal:8088/proxy/application_1435943097882_0019/<br />
15/07/07 13:24:30 INFO mapreduce.Job: Running job: job_1435943097882_0019 15/07/07 13:24:31 INFO mapreduce.Job: Job job_1435943097882_0019 running in uber mode : false<br />
15/07/07 13:24:31 INFO mapreduce.Job: map 0% reduce 0% 15/07/07 13:24:59 INFO mapreduce.Job: Job job_1435943097882_0019 failed with state FAILED due to: Vertex failed, vertexName=initialmap, vertexId=vertex_1435943097882_0019_1_00, diagnostics=[Task failed, taskId=task_1435943097882_0019_1_00_000005, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.io.EOFException at java.io.DataInputStream.readFully(DataInputStream.java:197)<br />
at org.apache.hadoop.io.Text.readWithKnownLength(Text.java:319) at org.apache.hadoop.io.Text.readFields(Text.java:291)<br />
at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:71) at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)<br />
at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:142) at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)<br />
at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)<br />
at org.apache.tez.mapreduce.combine.MRCombiner.runNewCombiner(MRCombiner.java:191) at org.apache.tez.mapreduce.combine.MRCombiner.combine(MRCombiner.java:115)<br />
at org.apache.tez.runtime.library.common.sort.impl.ExternalSorter.runCombineProcessor(ExternalSorter.java:285) at org.apache.tez.runtime.library.common.sort.impl.PipelinedSorter.spill(PipelinedSorter.java:463)<br />
at org.apache.tez.runtime.library.common.sort.impl.PipelinedSorter.sort(PipelinedSorter.java:219) at org.apache.tez.runtime.library.common.sort.impl.PipelinedSorter.collect(PipelinedSorter.java:311)<br />
at org.apache.tez.runtime.library.common.sort.impl.PipelinedSorter.write(PipelinedSorter.java:267) at org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput$1.write(OrderedPartitionedKVOutput.java:164)<br />
at org.apache.tez.mapreduce.processor.map.MapProcessor$NewOutputCollector.write(MapProcessor.java:363) at org.apache.tez.mapreduce.hadoop.mapreduce.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:90)<br />
at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112) at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:47)<br />
at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:36) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)<br />
at org.apache.tez.mapreduce.processor.map.MapProcessor.runNewMapper(MapProcessor.java:237) at org.apache.tez.mapreduce.processor.map.MapProcessor.run(MapProcessor.java:124)<br />
at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:345) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)<br />
at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171) at java.security.AccessController.doPrivileged(Native Method)<br />
at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)<br />
at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)<br />
at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:262)<br />
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)<br />
at java.lang.Thread.run(Thread.java:745) ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:89, Vertex vertex_1435943097882_0019_1_00 [initialmap] killed/failed due to:null]. Vertex killed, vertexName=finalreduce, vertexId=vertex_1435943097882_0019_1_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:15, Vertex vertex_1435943097882_0019_1_01 [finalreduce] killed/failed due to:null]. DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1<br />
15/07/07 13:24:59 INFO mapreduce.Job: Counters: 0 {quote}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2600">TEZ-2600</a> | <em>Major</em> | <strong>When used with HDFS federation(viewfs) ,tez will throw a error</strong></li>
</ul>
<p>When I execute the exapmle of tez,orderedwordcount ,Tez throw a error {code} java.lang.IllegalArgumentException: Wrong FS: hdfs://SunshineNameNode2/user/wang/tez-0.6.0.tar.gz, expected: viewfs://nsX/ at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645) at org.apache.hadoop.fs.viewfs.ViewFileSystem.getUriPath(ViewFileSystem.java:117) at org.apache.hadoop.fs.viewfs.ViewFileSystem.getFileStatus(ViewFileSystem.java:346) at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1413) at org.apache.tez.client.TezClientUtils.getLRFileStatus(TezClientUtils.java:130) at org.apache.tez.client.TezClientUtils.setupTezJarsLocalResources(TezClientUtils.java:179) at org.apache.tez.client.TezClient.getTezJarResources(TezClient.java:757) at org.apache.tez.client.TezClient.submitDAGApplication(TezClient.java:725) at org.apache.tez.client.TezClient.submitDAGApplication(TezClient.java:703) at org.apache.tez.client.TezClient.submitDAG(TezClient.java:383) at org.apache.tez.examples.OrderedWordCount.run(OrderedWordCount.java:208) at org.apache.tez.examples.OrderedWordCount.run(OrderedWordCount.java:232) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.tez.examples.OrderedWordCount.main(OrderedWordCount.java:240) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72) at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:145) at org.apache.tez.examples.ExampleDriver.main(ExampleDriver.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)</p>
<p>{code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2599">TEZ-2599</a> | <em>Major</em> | <strong>Dont send obsoleted data movement events to tasks</strong></li>
</ul>
<p>Since events are sent in bulk and in sequential order to tasks they can end up getting a bunch of data movement events in round 1 and then an input failed event in round 2. The task may end up scheduling fetches for these obsoleted events before round 2, leading to wastage. Given that an input failed event and data movement event can be matched via their source task attempt ids, the AM can use this match to obsolete the data movements events ahead of time and not send them to the tasks in the first place. The input failed events still need to be sent to the tasks, so that they can obsolete any data movements events that they may have received much earlier from the failed task attempt.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2588">TEZ-2588</a> | <em>Trivial</em> | <strong>Improper argument name</strong></li>
</ul>
<p>TezVertexID.java {code} public static TezVertexID fromString(String taskIdStr) { // should be vertexIdStr {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2579">TEZ-2579</a> | <em>Major</em> | <strong>Incorrect comparison of TaskAttemptId</strong></li>
</ul>
<p>TaskImpl#AttemptSucceededTransition {code} // issue kill to all other attempts for (TaskAttempt attempt : task.attempts.values()) { if (attempt.getID() != task.successfulAttempt &amp;&amp; // should use !equals !attempt.isFinished()) { // but it won't affect the state machine transition, because the successful task attempt should already complete. {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2575">TEZ-2575</a> | <em>Major</em> | <strong>Handle KeyValue pairs size which do not fit in a single block</strong></li>
</ul>
<p>In the present implementation, the available buffer is divided into blocks (specified in the constructor for pipeline sort). and a linked list of these block byte buffers is maintained. A span is created out of the buffers. The present logic, doesnot handle scenario where a single key-value pair size doesnot fit into any of the blocks. example if 1mb total memory is divided into 4 blocks, (256 kb each), if a single KV pair is greater than the blocksize(~ignoring meta data size), then it fails with buffer exceptions.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2568">TEZ-2568</a> | <em>Blocker</em> | <strong>V_INPUT_DATA_INFORMATION may happen after vertex is initialized</strong></li>
</ul>
<p>{code} 2015-06-19 15:57:28,462 ERROR [Dispatcher thread: Central] impl.VertexImpl: Can't handle Invalid event V_INPUT_DATA_INFORMATION on vertex Map 2 with vertexId vertex_1434754502979_0002_2_00 at current state INITED org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: V_INPUT_DATA_INFORMATION at INITED at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305) at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) at org.apache.tez.state.StateMachineTez.doTransition(StateMachineTez.java:57) at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1799) at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:198) at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1963) at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1949) at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183) at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114) at java.lang.Thread.run(Thread.java:722) {code}</p>
<p>Vertex move to INITED as long as its parallelism is determined, no null edges and root inputs are initialized. RootInputDataInformation handling is not a precondition of vertex move to INITED. We can't wait for all the V_INPUT_DATA_INFORMATION events available in INITIALIZING state, because it is not know how many V_INPUT_DATA_INFORMATION we may receive, it is determined by VM. So will allow V_INPUT_DATA_INFORMATION happens when vertex is initialized.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2567">TEZ-2567</a> | <em>Major</em> | <strong>Tez UI: download dag data does not work within ambari</strong></li>
</ul>
<p>downloading of dag data from the ui while using a ambari fails. This is due to the fact that $.ajax call does not know how to parse the data as the content-type headers are removed by the ambari proxy (this was a getJSON call later changed to ajax call as we needed to pass the credentials).</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2566">TEZ-2566</a> | <em>Major</em> | <strong>Allow TaskAttemptFinishedEvent without TaskAttemptStartedEvent when it is KILLED/FAILED</strong></li>
</ul>
<p>TEZ-2304 allow logging TaskAttempFinishedEvent even without TaskAttemptStartedEvent but don't change the logic in Task#restoreFromEvent. Task attempt is possible to be KILLED/FAILED before it is started, but not possible to be SUCCEEDED without started.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2561">TEZ-2561</a> | <em>Major</em> | <strong>Port for TaskAttemptListenerImpTezDag should be configurable</strong></li>
</ul>
<p>Noticed sporadic DAG failures in our ec2 test environment. Tasks failing with that: {noformat} 2015-06-17 11:19:51,064 INFO [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s). 2015-06-17 11:19:51,064 INFO [main] impl.MetricsSystemImpl: TezTask metrics system started 2015-06-17 11:19:51,259 INFO [TezChild] task.ContainerReporter: Attempting to fetch new task 2015-06-17 11:20:11,311 INFO [TezChild] ipc.Client: Retrying connect to server: ip-10-149-102-100.ec2.internal/10.149.102.100:60630. Already tried 0 time(s); maxRetries=5 2015-06-17 11:20:31,312 INFO [TezChild] ipc.Client: Retrying connect to server: ip-10-149-102-100.ec2.internal/10.149.102.100:60630. Already tried 1 time(s); maxRetries=5 2015-06-17 11:20:51,313 INFO [TezChild] ipc.Client: Retrying connect to server: ip-10-149-102-100.ec2.internal/10.149.102.100:60630. Already tried 2 time(s); maxRetries=5 2015-06-17 11:21:11,314 INFO [TezChild] ipc.Client: Retrying connect to server: ip-10-149-102-100.ec2.internal/10.149.102.100:60630. Already tried 3 time(s); maxRetries=5 2015-06-17 11:21:31,315 INFO [TezChild] ipc.Client: Retrying connect to server: ip-10-149-102-100.ec2.internal/10.149.102.100:60630. Already tried 4 time(s); maxRetries=5 2015-06-17 11:21:51,317 INFO [main] impl.MetricsSystemImpl: Stopping TezTask metrics system... 2015-06-17 11:21:51,318 INFO [main] impl.MetricsSystemImpl: TezTask metrics system stopped. 2015-06-17 11:21:51,318 INFO [main] impl.MetricsSystemImpl: TezTask metrics system shutdown complete. {noformat}</p>
<p>From the AppMaster: {noformat} Created DAGAppMaster for application appattempt_1434553606315_0022_000001 2015-06-17 11:19:43,655 INFO [Socket Reader #1 for port 60630] ipc.Server: Starting Socket Reader #1 for port 60630 2015-06-17 11:19:43,656 INFO [Socket Reader #1 for port 31001] ipc.Server: Starting Socket Reader #1 for port 31001 2015-06-17 11:19:43,713 WARN [ServiceThread:org.apache.tez.dag.history.HistoryEventHandler] conf.Configuration: mapred-site.xml:an attempt to override final parameter: mapreduce.cluster.local.dir; Ignoring. {noformat}</p>
<p>[~hitesh] mentioned its likely to be the TaskAttemptListenerImpTezDag which starts on that port. Would be nice if the port(-range) can be configured!!!</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2560">TEZ-2560</a> | <em>Major</em> | <strong>fix tex-ui build for maven 3.3+</strong></li>
</ul>
<p>currently tez-ui build fails if mvn version is 3.3 due to the frontend-maven-plugin. this is fixed in 0.0.23 version of the plugin but it fails on maven version below 3.1</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2559">TEZ-2559</a> | <em>Major</em> | <strong>tez-ui fails compilation due to version dependency of frontend-maven-plugin</strong></li>
</ul>
<p>{noformat} [INFO] --- build-helper-maven-plugin:1.8:maven-version (maven-version) @ tez-ui --- [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO][INFO] tez ............................................... SUCCESS [0.941s][INFO] tez-api ........................................... SUCCESS [5.069s][INFO] tez-common ........................................ SUCCESS [0.389s][INFO] tez-runtime-internals ............................. SUCCESS [0.694s][INFO] tez-runtime-library ............................... SUCCESS [1.701s][INFO] tez-mapreduce ..................................... SUCCESS [0.910s][INFO] tez-examples ...................................... SUCCESS [0.190s][INFO] tez-dag ........................................... SUCCESS [3.808s][INFO] tez-tests ......................................... SUCCESS [0.643s][INFO] tez-ui ............................................ FAILURE [0.044s][INFO] tez-plugins ....................................... SKIPPED [INFO] tez-yarn-timeline-history ......................... SKIPPED [INFO] tez-yarn-timeline-history-with-acls ............... SKIPPED [INFO] tez-history-parser ................................ SKIPPED [INFO] tez-mbeans-resource-calculator .................... SKIPPED [INFO] tez-tools ......................................... SKIPPED [INFO] tez-dist .......................................... SKIPPED [INFO] Tez ............................................... SKIPPED [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time: 14.665s [INFO] Finished at: Wed Jun 17 12:17:04 IST 2015 [INFO] Final Memory: 56M/356M [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.23:install-node-and-npm (install node and npm) on project tez-ui: The plugin com.github.eirslett:frontend-maven-plugin:0.0.23 requires Maven version 3.1.0 -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginIncompatibleException {noformat}</p>
<p>build fails on mac and linux.</p>
<p></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2558">TEZ-2558</a> | <em>Major</em> | <strong>Upload additional Tez images</strong></li>
</ul>
<p>Converted current to eps format. Additional powered by images provided by Sally Khudairi.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2554">TEZ-2554</a> | <em>Major</em> | <strong>Tez UI: View log link does not correctly propagate login crendential to read log from yarn web.</strong></li>
</ul>
<p>Append &quot;user.name=<am user>&quot; to the view/download logs url as a query param.</p>
<p>Sample url: http://address:19888/jobhistory/logs/address:45454/container_e18_1434089649193_0001_01_000002/container_e18_1434089649193_0001_01_000002/hrt_qa?user.name=hrt_qa</p>
<p>Reported by [~tassapola] offline.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2552">TEZ-2552</a> | <em>Major</em> | <strong>CRC errors can cause job to run for very long time in large jobs</strong></li>
</ul>
<p>Ran a fairly large job at 10 TB scale which had 1009 reducers.</p>
<p>One of the machine had bad disk and NM did not delist that disk. Machine hosting NM has disk issues (sdf &amp; sde holds shuffle data). exceptions.</p>
<p>{noformat} Info fld=0x8960894 sd 6:0:5:0: [sdf] Add. Sense: Unrecovered read error sd 6:0:5:0: [sdf] CDB: Read(10): 28 00 08 96 08 90 00 00 08 00 end_request: critical medium error, dev sdf, sector 144050320 sd 6:0:5:0: [sdf] Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE sd 6:0:5:0: [sdf] Sense Key : Medium Error [current] Info fld=0x895a2b9 sd 6:0:5:0: [sdf] Add. Sense: Unrecovered read error sd 6:0:5:0: [sdf] CDB: Read(10): 28 00 08 95 a2 b8 00 00 08 00 end_request: critical medium error, dev sdf, sector 144024248 sd 6:0:5:0: [sdf] Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE sd 6:0:5:0: [sdf] Sense Key : Medium Error [current] Info fld=0x895a2b9 sd 6:0:5:0: [sdf] Add. Sense: Unrecovered read error sd 6:0:5:0: [sdf] CDB: Read(10): 28 00 08 95 a2 b8 00 00 08 00 end_request: critical medium error, dev sdf, sector 144024248 sd 6:0:5:0: [sdf] Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE sd 6:0:5:0: [sdf] Sense Key : Medium Error [current] Info fld=0x8849edb sd 6:0:5:0: [sdf] Add. Sense: Unrecovered read error sd 6:0:5:0: [sdf] CDB: Read(10): 28 00 08 84 9e d8 00 00 08 00 end_request: critical medium error, dev sdf, sector 142909144 sd 6:0:5:0: [sdf] Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE sd 6:0:5:0: [sdf] Sense Key : Medium Error [current] Info fld=0x8849edb sd 6:0:5:0: [sdf] Add. Sense: Unrecovered read error sd 6:0:5:0: [sdf] CDB: Read(10): 28 00 08 84 9e d8 00 00 08 00 end_request: critical medium error, dev sdf, sector 142909144 {noformat}</p>
<p>In-memory fetches start throwing CRC as follows.</p>
<p>{noformat} 2015-06-11 01:01:03,728 INFO [ShuffleAndMergeRunner [Map_11]] orderedgrouped.ShuffleScheduler: PendingHosts=[] 2015-06-11 01:01:03,730 INFO [Fetcher [Map_11] #0] http.HttpConnection: for url=http://cn056-10.l42scl.hortonworks.com:13562/mapOutput?job=job_1433813751839_0124&amp;reduce=3&amp;map=attempt_1433813751839_0124_1_04_000446_0_10027&amp;keepAlive=true sent hash and receievd reply 0 ms 2015-06-11 01:01:03,730 INFO [Fetcher [Map_11] #0] orderedgrouped.FetcherOrderedGrouped: fetcher#439 about to shuffle output of map InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=446], attemptNumber=0, pathComponent=attempt_1433813751839_0124_1_04_000446_0_10027, fetchTypeInfo=FINAL_MERGE_ENABLED, spillEventId=-1] decomp: 45475 len: 23974 to MEMORY 2015-06-11 01:01:07,206 INFO [Fetcher [Map_11] #0] impl.IFileInputStream: CurrentOffset=2510, offset=2510, off=2502, dataLength=23966, origLen=21456, len=21456, length=23970, checksumSize=4 2015-06-11 01:01:07,207 INFO [Fetcher [Map_11] #0] impl.IFileInputStream: CurrentOffset=2510, offset=2510, off=0, dataLength=23966, origLen=21456, len=21456, length=23970, checksumSize=4 2015-06-11 01:01:07,207 WARN [Fetcher [Map_11] #0] orderedgrouped.FetcherOrderedGrouped: Failed to shuffle output of InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=446], attemptNumber=0, pathComponent=attempt_1433813751839_0124_1_04_000446_0_10027, fetchTypeInfo=FINAL_MERGE_ENABLED, spillEventId=-1] from cn056-10.l42scl.hortonworks.com:13562 org.apache.hadoop.fs.ChecksumException: Checksum Error: CurrentOffset=2510, offset=2510, off=2502, dataLength=23966, origLen=21456, len=21456, length=23970, checksumSize=4 at org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.doRead(IFileInputStream.java:255) at org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.read(IFileInputStream.java:185) at org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:127) at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:98) at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85) at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192) at org.apache.tez.runtime.library.common.sort.impl.IFile$Reader.readToMemory(IFile.java:619) at org.apache.tez.runtime.library.common.shuffle.ShuffleUtils.shuffleToMemory(ShuffleUtils.java:113) at org.apache.tez.runtime.library.common.shuffle.orderedgrouped.FetcherOrderedGrouped.copyMapOutput(FetcherOrderedGrouped.java:471) at org.apache.tez.runtime.library.common.shuffle.orderedgrouped.FetcherOrderedGrouped.copyFromHost(FetcherOrderedGrouped.java:267) at org.apache.tez.runtime.library.common.shuffle.orderedgrouped.FetcherOrderedGrouped.fetchNext(FetcherOrderedGrouped.java:164) at org.apache.tez.runtime.library.common.shuffle.orderedgrouped.FetcherOrderedGrouped.callInternal(FetcherOrderedGrouped.java:177) at org.apache.tez.runtime.library.common.shuffle.orderedgrouped.FetcherOrderedGrouped.callInternal(FetcherOrderedGrouped.java:52) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) {noformat}</p>
<p>TaskAttemptImpl didn't fail it due to the following code</p>
<p>{noformat} float failureFraction = ((float) attempt.uniquefailedOutputReports.size()) / outputFailedEvent.getConsumerTaskNumber(); {noformat}</p>
<p>In this case, reducer ran in 180 slot waves. So even if all 180 tasks report the error, it would be around 180/1009 = 0.17 (which is less than 0.25 MAX_ALLOWED_OUTPUT_FAILURES_FRACTION) and the job runs for ever (killed the job after 2 hours; normally run in couple of minutes)</p>
<p>In fetcher side, reducer state would be healthy and it would continue to wait.</p>
<p>Env: Tez master &amp; Hive master Ref: Query_88 @ 10 TB scale.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2548">TEZ-2548</a> | <em>Major</em> | <strong>TezClient submitDAG can hang if the AM is in the process of shutting down</strong></li>
</ul>
<p>submitDAG and serviceStop are both synchronized causing submitDAG to be locked out during the shutdown process.</p>
<p>Seen by [~gopalv]</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2546">TEZ-2546</a> | <em>Major</em> | <strong>Tez UI: Fetch hive query text from timeline if dagInfo is not set</strong></li>
</ul>
<p>Followup to TEZ-2453, to make UI backward compatible and display hive queries in order versions.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2545">TEZ-2545</a> | <em>Major</em> | <strong>It is not necessary to start the vertex group commit when DAG is in TERMINATING</strong></li>
</ul>
<p>{noformat} Failed</p>
<p>org.apache.tez.dag.app.dag.impl.TestCommit.testDAGKilledWhileRunning_OnVertexSuccess</p>
<p>Failing for the past 2 builds (Since Unstable#3372 ) 运行时间：5 秒 Error Message</p>
<p>test timed out after 5000 milliseconds Stacktrace</p>
<p>java.lang.Exception: test timed out after 5000 milliseconds at java.lang.Thread.sleep(Native Method) at org.apache.tez.dag.app.dag.impl.TestCommit.waitUntil(TestCommit.java:355) at org.apache.tez.dag.app.dag.impl.TestCommit.testDAGKilledWhileRunning_OnVertexSuccess(TestCommit.java:1652)</p>
<p>Standard Output</p>
<p>2015-06-09 04:12:25,778 INFO [Thread-43] impl.TestCommit (TestCommit.java:createDAGPlan(401)) - Setting up group dag plan 2015-06-09 04:12:25,782 INFO [Thread-43] event.AsyncDispatcher (AsyncDispatcher.java:register(200)) - Registering class org.apache.tez.dag.app.dag.event.CallableEventType for class org.apache.tez.dag.app.dag.impl.CallableEventDispatcher 2015-06-09 04:12:25,782 INFO [Thread-43] event.AsyncDispatcher (AsyncDispatcher.java:register(200)) - Registering class org.apache.tez.dag.app.dag.event.TaskEventType for class org.apache.tez.dag.app.dag.impl.TestCommit$TaskEventDispatcher 2015-06-09 04:12:25,782 INFO [Thread-43] event.AsyncDispatcher (AsyncDispatcher.java:register(200)) - Registering class org.apache.tez.dag.app.dag.event.TaskAttemptEventType for class org.apache.tez.dag.app.dag.impl.TestCommit$TaskAttemptEventDispatcher 2015-06-09 04:12:25,782 INFO [Thread-43] event.AsyncDispatcher (AsyncDispatcher.java:register(200)) - Registering class org.apache.tez.dag.app.dag.event.VertexEventType for class org.apache.tez.dag.app.dag.impl.TestCommit$VertexEventDispatcher 2015-06-09 04:12:25,782 INFO [Thread-43] event.AsyncDispatcher (AsyncDispatcher.java:register(200)) - Registering class org.apache.tez.dag.app.dag.event.DAGEventType for class org.apache.tez.dag.app.dag.impl.TestCommit$DagEventDispatcher 2015-06-09 04:12:25,783 INFO [Thread-43] event.AsyncDispatcher (AsyncDispatcher.java:register(200)) - Registering class org.apache.tez.dag.app.dag.event.DAGAppMasterEventType for class org.apache.tez.dag.app.dag.impl.TestCommit$DAGFinishEventHandler 2015-06-09 04:12:25,784 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:setAdditionalOutputs(4497)) - setting additional outputs for vertex vertex2 2015-06-09 04:12:25,784 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:setAdditionalOutputs(4497)) - setting additional outputs for vertex vertex1 2015-06-09 04:12:25,786 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:setAdditionalOutputs(4497)) - setting additional outputs for vertex vertex3 2015-06-09 04:12:25,787 INFO [Thread-43] impl.DAGImpl (DAGImpl.java:assignDAGScheduler(1488)) - Using DAG Scheduler: org.apache.tez.dag.app.dag.impl.DAGSchedulerNaturalOrder 2015-06-09 04:12:25,788 INFO [Thread-43] impl.DAGImpl (DAGImpl.java:handle(1100)) - dag_100_0001_1 transitioned from NEW to INITED 2015-06-09 04:12:25,789 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:handle(1100)) - dag_100_0001_1 transitioned from INITED to RUNNING 2015-06-09 04:12:25,790 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:assignVertexManager(2469)) - Setting vertexManager to ImmediateStartVertexManager for vertex_100_0001_1_01 [vertex1] 2015-06-09 04:12:25,790 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handleInitEvent(3246)) - Creating 1 tasks for vertex: vertex_100_0001_1_01 [vertex1] 2015-06-09 04:12:25,790 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handleInitEvent(3258)) - Directly initializing vertex: vertex_100_0001_1_01 [vertex1] 2015-06-09 04:12:25,790 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:initializeCommitters(2131)) - Invoking committer inits for vertex, vertexId=vertex_100_0001_1_01 [vertex1] 2015-06-09 04:12:25,790 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:initializeCommitters(2143)) - Instantiating committer for output=v12Out, vertexId=vertex_100_0001_1_01 [vertex1], committerClass=org.apache.tez.dag.app.dag.impl.TestCommit$CountingOutputCommitter 2015-06-09 04:12:25,791 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:run(2161)) - Invoking committer init for output=v12Out, vertexId=vertex_100_0001_1_01 [vertex1] 2015-06-09 04:12:25,791 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:run(2165)) - Invoking committer setup for output=v12Out, vertexId=vertex_100_0001_1_01 [vertex1] 2015-06-09 04:12:25,791 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_01 [vertex1] transitioned from NEW to INITED due to event V_INIT 2015-06-09 04:12:25,791 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:assignVertexManager(2469)) - Setting vertexManager to ImmediateStartVertexManager for vertex_100_0001_1_00 [vertex2] 2015-06-09 04:12:25,792 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handleInitEvent(3246)) - Creating 1 tasks for vertex: vertex_100_0001_1_00 [vertex2] 2015-06-09 04:12:25,792 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handleInitEvent(3258)) - Directly initializing vertex: vertex_100_0001_1_00 [vertex2] 2015-06-09 04:12:25,792 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:initializeCommitters(2131)) - Invoking committer inits for vertex, vertexId=vertex_100_0001_1_00 [vertex2] 2015-06-09 04:12:25,792 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:initializeCommitters(2143)) - Instantiating committer for output=v12Out, vertexId=vertex_100_0001_1_00 [vertex2], committerClass=org.apache.tez.dag.app.dag.impl.TestCommit$CountingOutputCommitter 2015-06-09 04:12:25,793 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:run(2161)) - Invoking committer init for output=v12Out, vertexId=vertex_100_0001_1_00 [vertex2] 2015-06-09 04:12:25,794 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:run(2165)) - Invoking committer setup for output=v12Out, vertexId=vertex_100_0001_1_00 [vertex2] 2015-06-09 04:12:25,794 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_00 [vertex2] transitioned from NEW to INITED due to event V_INIT 2015-06-09 04:12:25,794 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_01 [vertex1] transitioned from INITED to RUNNING due to event V_START 2015-06-09 04:12:25,794 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_00 [vertex2] transitioned from INITED to RUNNING due to event V_START 2015-06-09 04:12:25,795 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:assignVertexManager(2462)) - Setting vertexManager to ShuffleVertexManager for vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,823 INFO [AsyncDispatcher event handler] vertexmanager.ShuffleVertexManager (ShuffleVertexManager.java:initialize(813)) - Shuffle Vertex Manager: settings minFrac:0.25 maxFrac:0.75 auto:false desiredTaskIput:104857600 minTasks:1 2015-06-09 04:12:25,823 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handleInitEvent(3246)) - Creating 1 tasks for vertex: vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,826 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handleInitEvent(3258)) - Directly initializing vertex: vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,826 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:initializeCommitters(2131)) - Invoking committer inits for vertex, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,826 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:initializeCommitters(2143)) - Instantiating committer for output=v3Out, vertexId=vertex_100_0001_1_02 [vertex3], committerClass=org.apache.tez.dag.app.dag.impl.TestCommit$CountingOutputCommitter 2015-06-09 04:12:25,827 INFO [App Shared Pool - #0] impl.ImmediateStartVertexManager (ImmediateStartVertexManager.java:scheduleTasks(99)) - Starting 1 in vertex1 2015-06-09 04:12:25,827 INFO [App Shared Pool - #1] impl.ImmediateStartVertexManager (ImmediateStartVertexManager.java:scheduleTasks(99)) - Starting 1 in vertex2 2015-06-09 04:12:25,827 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:run(2161)) - Invoking committer init for output=v3Out, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,827 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:run(2165)) - Invoking committer setup for output=v3Out, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,828 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_02 [vertex3] transitioned from NEW to INITED due to event V_INIT 2015-06-09 04:12:25,829 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:transition(3407)) - Source vertex started: vertex_100_0001_1_01 for vertex: vertex_100_0001_1_02 [vertex3] numStartedSources: 1 numSources: 2 2015-06-09 04:12:25,829 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:transition(3412)) - Cannot start vertex: vertex_100_0001_1_02 [vertex3] numStartedSources: 1 numSources: 2 2015-06-09 04:12:25,829 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:transition(3407)) - Source vertex started: vertex_100_0001_1_00 for vertex: vertex_100_0001_1_02 [vertex3] numStartedSources: 2 numSources: 2 2015-06-09 04:12:25,829 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:startIfPossible(3293)) - Triggering start event for vertex: vertex_100_0001_1_02 [vertex3] with distanceFromRoot: 1 2015-06-09 04:12:25,830 INFO [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:handle(899)) - task_100_0001_1_01_000000 Task Transitioned from NEW to SCHEDULED due to event T_SCHEDULE 2015-06-09 04:12:25,830 INFO [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:handle(899)) - task_100_0001_1_00_000000 Task Transitioned from NEW to SCHEDULED due to event T_SCHEDULE 2015-06-09 04:12:25,830 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_02 [vertex3] transitioned from INITED to RUNNING due to event V_START 2015-06-09 04:12:25,831 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:transition(3817)) - Num completed Tasks for vertex_100_0001_1_01 [vertex1] : 1 2015-06-09 04:12:25,831 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:checkTasksForCompletion(1955)) - Checking tasks for vertex completion for vertex_100_0001_1_01 [vertex1], numTasks=1, failedTaskCount=0, killedTaskCount=0, successfulTaskCount=1, completedTaskCount=1, commitInProgress=0, terminationCause=null 2015-06-09 04:12:25,832 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:checkTasksForCompletion(1983)) - All tasks are succeeded, vertex:vertex_100_0001_1_01 [vertex1] 2015-06-09 04:12:25,832 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_01 [vertex1] transitioned from RUNNING to SUCCEEDED due to event V_TASK_COMPLETED 2015-06-09 04:12:25,832 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:transition(1883)) - Vertex vertex_100_0001_1_01 [vertex1] completed., numCompletedVertices=1, numSuccessfulVertices=1, numFailedVertices=0, numKilledVertices=0, numVertices=3 2015-06-09 04:12:25,834 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:checkVerticesForCompletion(1178)) - Checking vertices for DAG completion, numCompletedVertices=1, numSuccessfulVertices=1, numFailedVertices=0, numKilledVertices=0, numVertices=3, commitInProgress=0, terminationCause=null 2015-06-09 04:12:25,833 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:transition(3817)) - Num completed Tasks for vertex_100_0001_1_00 [vertex2] : 1 2015-06-09 04:12:25,834 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:checkTasksForCompletion(1955)) - Checking tasks for vertex completion for vertex_100_0001_1_00 [vertex2], numTasks=1, failedTaskCount=0, killedTaskCount=0, successfulTaskCount=1, completedTaskCount=1, commitInProgress=0, terminationCause=null 2015-06-09 04:12:25,834 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:checkTasksForCompletion(1983)) - All tasks are succeeded, vertex:vertex_100_0001_1_00 [vertex2] 2015-06-09 04:12:25,834 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_00 [vertex2] transitioned from RUNNING to SUCCEEDED due to event V_TASK_COMPLETED 2015-06-09 04:12:25,834 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:transition(3817)) - Num completed Tasks for vertex_100_0001_1_02 [vertex3] : 1 2015-06-09 04:12:25,834 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:checkTasksForCompletion(1955)) - Checking tasks for vertex completion for vertex_100_0001_1_02 [vertex3], numTasks=1, failedTaskCount=0, killedTaskCount=0, successfulTaskCount=1, completedTaskCount=1, commitInProgress=0, terminationCause=null 2015-06-09 04:12:25,835 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:checkTasksForCompletion(1983)) - All tasks are succeeded, vertex:vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,836 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:commitOrFinish(1907)) - Invoking committer commit for vertex, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,836 INFO [Thread-43] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_02 [vertex3] transitioned from RUNNING to COMMITTING due to event V_TASK_COMPLETED 2015-06-09 04:12:25,836 INFO [Thread-43] impl.DAGImpl (DAGImpl.java:handle(1100)) - dag_100_0001_1 transitioned from RUNNING to TERMINATING 2015-06-09 04:12:25,836 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:25,834 INFO [App Shared Pool - #1] vertexmanager.ShuffleVertexManager (ShuffleVertexManager.java:onVertexStarted(467)) - OnVertexStarted vertex: vertex3 with 2 source tasks and 1 pending tasks 2015-06-09 04:12:25,836 INFO [App Shared Pool - #0] impl.VertexImpl (VertexImpl.java:run(1930)) - Invoking committer commit for output=v3Out, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,838 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:vertexSucceeded(1943)) - All members of group: uv12 are succeeded. Commiting outputs 2015-06-09 04:12:25,838 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:transition(1883)) - Vertex vertex_100_0001_1_00 [vertex2] completed., numCompletedVertices=2, numSuccessfulVertices=2, numFailedVertices=0, numKilledVertices=0, numVertices=3 2015-06-09 04:12:25,838 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:checkVerticesForCompletion(1178)) - Checking vertices for DAG completion, numCompletedVertices=2, numSuccessfulVertices=2, numFailedVertices=0, numKilledVertices=0, numVertices=3, commitInProgress=1, terminationCause=DAG_KILL 2015-06-09 04:12:25,838 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:transition(3710)) - Vertex received Kill while in COMMITTING state, terminationCause=DAG_KILL, vertex=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,838 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:cancelCommits(3979)) - Canceling commit of output:v3Out, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,838 INFO [App Shared Pool - #2] vertexmanager.ShuffleVertexManager (ShuffleVertexManager.java:handleVertexStateUpdate(837)) - Received configured notification : CONFIGURED for vertex: vertex1 in vertex: vertex3 2015-06-09 04:12:25,838 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_02 [vertex3] transitioned from COMMITTING to TERMINATING due to event V_TERMINATE 2015-06-09 04:12:25,839 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:commitCompleted(3959)) - Commit failed for output:v3Out, vertexId=vertex_100_0001_1_02 [vertex3], java.util.concurrent.CancellationException at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:250) at java.util.concurrent.FutureTask.get(FutureTask.java:111) at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:132) at com.google.common.util.concurrent.Futures$6.run(Futures.java:974) at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:253) at com.google.common.util.concurrent.ExecutionList$RunnableExecutorPair.execute(ExecutionList.java:149) at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:134) at com.google.common.util.concurrent.ListenableFutureTask.done(ListenableFutureTask.java:86) at java.util.concurrent.FutureTask$Sync.innerCancel(FutureTask.java:322) at java.util.concurrent.FutureTask.cancel(FutureTask.java:104) at org.apache.tez.dag.app.dag.impl.VertexImpl.cancelCommits(VertexImpl.java:3980) at org.apache.tez.dag.app.dag.impl.VertexImpl.access$5600(VertexImpl.java:199) at org.apache.tez.dag.app.dag.impl.VertexImpl$VertexKilledWhileCommittingTransition.transition(VertexImpl.java:3713) at org.apache.tez.dag.app.dag.impl.VertexImpl$VertexKilledWhileCommittingTransition.transition(VertexImpl.java:3701) at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362) at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) at org.apache.tez.state.StateMachineTez.doTransition(StateMachineTez.java:57) at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1799) at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:198) at org.apache.tez.dag.app.dag.impl.TestCommit$VertexEventDispatcher.handle(TestCommit.java:168) at org.apache.tez.dag.app.dag.impl.TestCommit$VertexEventDispatcher.handle(TestCommit.java:162) at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175) at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108) at java.lang.Thread.run(Thread.java:722)</p>
<p>2015-06-09 04:12:25,839 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:checkCommitsForCompletion(2001)) - Checking commits for vertex completion for vertex_100_0001_1_02 [vertex3], numTasks=1, failedTaskCount=0, killedTaskCount=0, successfulTaskCount=1, completedTaskCount=1, commitInProgress=0, terminationCause=DAG_KILL 2015-06-09 04:12:25,840 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:finishWithTerminationCause(2038)) - Vertex did not succeed due to DAG_KILL, failedTasks:0 killedTasks:0 2015-06-09 04:12:25,840 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:abortVertex(3556)) - Invoking committer abort for vertex, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,840 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:run(3563)) - Invoking committer abort for output=v3Out, vertexId=vertex_100_0001_1_02 [vertex3] 2015-06-09 04:12:25,841 INFO [AsyncDispatcher event handler] impl.VertexImpl (VertexImpl.java:handle(1812)) - vertex_100_0001_1_02 [vertex3] transitioned from TERMINATING to KILLED due to event V_COMMIT_COMPLETED 2015-06-09 04:12:25,842 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:transition(1883)) - Vertex vertex_100_0001_1_02 [vertex3] completed., numCompletedVertices=3, numSuccessfulVertices=2, numFailedVertices=0, numKilledVertices=1, numVertices=3 2015-06-09 04:12:25,842 INFO [AsyncDispatcher event handler] impl.DAGImpl (DAGImpl.java:checkVerticesForCompletion(1178)) - Checking vertices for DAG completion, numCompletedVertices=3, numSuccessfulVertices=2, numFailedVertices=0, numKilledVertices=1, numVertices=3, commitInProgress=1, terminationCause=DAG_KILL 2015-06-09 04:12:25,838 INFO [App Shared Pool - #3] impl.DAGImpl (DAGImpl.java:call(1971)) - Committing output: v12Out 2015-06-09 04:12:25,840 INFO [App Shared Pool - #0] vertexmanager.ShuffleVertexManager (ShuffleVertexManager.java:handleVertexStateUpdate(837)) - Received configured notification : CONFIGURED for vertex: vertex2 in vertex: vertex3 2015-06-09 04:12:25,936 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:25,943 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,039 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,044 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,141 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,144 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,242 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,245 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,342 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,345 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,443 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,446 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,544 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,547 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,644 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,647 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,745 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,749 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,846 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,849 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:26,947 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:26,950 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,048 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,050 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,148 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,151 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,249 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,252 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,349 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,353 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,450 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,454 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,550 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,555 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,651 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,656 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,752 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,757 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,852 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,857 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:27,953 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:27,958 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,053 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,058 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,155 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,159 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,256 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,259 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,356 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,360 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,457 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,462 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,557 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,562 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,658 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,663 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,759 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,763 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,859 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,864 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:28,960 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:28,966 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,061 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,067 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,162 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,168 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,262 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,268 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,363 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,369 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,464 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,469 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,565 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,570 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,666 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,671 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,766 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,771 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,869 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,872 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:29,970 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:29,972 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,072 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,073 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,173 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,174 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,274 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,275 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,376 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,376 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,476 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,476 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,577 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,577 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,678 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,679 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,786 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED 2015-06-09 04:12:30,808 INFO [App Shared Pool - #3] impl.TestCommit (TestCommit.java:commitOutput(234)) - committing output:v12Out 2015-06-09 04:12:30,815 INFO [Thread-43] impl.TestCommit (TestCommit.java:waitUntil(353)) - Wait for dag go to state:KILLED Standard Error</p>
<p>java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at org.apache.tez.dag.app.dag.impl.TestCommit.waitUntil(TestCommit.java:355) at org.apache.tez.dag.app.dag.impl.TestCommit.testDAGKilledWhileRunning_OnVertexSuccess(TestCommit.java:1652) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74) {noformat}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2542">TEZ-2542</a> | <em>Minor</em> | <strong>TezDAGID fromString array length check</strong></li>
</ul>
<p>When invalid DAG Id passed</p>
<blockquote>
<p>java -cp tez-history-parser-0.8.0-SNAPSHOT-jar-with-dependencies.jar org.apache.tez.history.ATSImportTool --dagId dag_1433663662240_0001 --downloadDir /tmp/history/ java.lang.ArrayIndexOutOfBoundsException: 3 at org.apache.tez.dag.records.TezDAGID.fromString(TezDAGID.java:185) at org.apache.tez.history.ATSImportTool.<init>(ATSImportTool.java:133) at org.apache.tez.history.ATSImportTool.process(ATSImportTool.java:452) at org.apache.tez.history.ATSImportTool.main(ATSImportTool.java:476)</p>
</blockquote>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2534">TEZ-2534</a> | <em>Major</em> | <strong>Error handling summary event when shutting down AM</strong></li>
</ul>
<p>When AM is shutting down, it will close the summary stream, but there may be still some events in the queue which will cause exception when handling summary event. And this would cause the next AM fail to recover the running dag. One way to resolve this issue is to always drain the events in the queue before closing the summary stream (set drainEventsFlag as true), but this flag may be useful in unit test. {noformat} 2015-06-03 16:37:15,761 INFO [Thread-1] app.DAGAppMaster: DAGAppMasterShutdownHook invoked 2015-06-03 16:37:15,761 INFO [Thread-1] app.DAGAppMaster: DAGAppMaster received a signal. Signaling TaskScheduler 2015-06-03 16:37:15,761 INFO [Thread-1] rm.TaskSchedulerEventHandler: TaskScheduler notified that iSignalled was : true 2015-06-03 16:37:15,762 INFO [Thread-1] history.HistoryEventHandler: Stopping HistoryEventHandler 2015-06-03 16:37:15,762 INFO [Thread-1] recovery.RecoveryService: Stopping RecoveryService 2015-06-03 16:37:15,762 INFO [Thread-1] recovery.RecoveryService: Closing Summary Stream 2015-06-03 16:37:15,772 INFO [Thread-1] recovery.RecoveryService: Closing Output Stream for DAG dag_1433320263267_0019_1 2015-06-03 16:37:15,773 ERROR [Dispatcher thread: Central] recovery.RecoveryService: Error handling summary event, eventType=VERTEX_FINISHED java.nio.channels.ClosedChannelException at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1622) at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:104) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58) at java.io.DataOutputStream.write(DataOutputStream.java:107) at com.google.protobuf.CodedOutputStream.refreshBuffer(CodedOutputStream.java:833) at com.google.protobuf.CodedOutputStream.flush(CodedOutputStream.java:843) at com.google.protobuf.AbstractMessageLite.writeDelimitedTo(AbstractMessageLite.java:91) at org.apache.tez.dag.history.events.VertexFinishedEvent.toSummaryProtoStream(VertexFinishedEvent.java:207) at org.apache.tez.dag.history.recovery.RecoveryService.handleSummaryEvent(RecoveryService.java:373) at org.apache.tez.dag.history.recovery.RecoveryService.handle(RecoveryService.java:285) at org.apache.tez.dag.history.HistoryEventHandler.handleCriticalEvent(HistoryEventHandler.java:105) at org.apache.tez.dag.app.dag.impl.VertexImpl.logJobHistoryVertexCompletedHelper(VertexImpl.java:1890) at org.apache.tez.dag.app.dag.impl.VertexImpl.logJobHistoryVertexFinishedEvent(VertexImpl.java:1869) at org.apache.tez.dag.app.dag.impl.VertexImpl.finished(VertexImpl.java:2107) at org.apache.tez.dag.app.dag.impl.VertexImpl.finished(VertexImpl.java:2125) at org.apache.tez.dag.app.dag.impl.VertexImpl.checkTasksForCompletion(VertexImpl.java:1989) at org.apache.tez.dag.app.dag.impl.VertexImpl$TaskCompletedTransition.transition(VertexImpl.java:3833) at org.apache.tez.dag.app.dag.impl.VertexImpl$TaskCompletedTransition.transition(VertexImpl.java:1) at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385) at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) at org.apache.tez.state.StateMachineTez.doTransition(StateMachineTez.java:57) at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1799) at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1) at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1954) at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1) at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183) at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114) at java.lang.Thread.run(Thread.java:745) 2015-06-03 16:37:15,775 ERROR [Dispatcher thread: Central] recovery.RecoveryService: Adding a flag to ensure next AM attempt does not start up, flagFile=hdfs://localhost:58857/tmp/owc-staging-dir/.tez/application_1433320263267_0019/recovery/1/RecoveryFatalErrorOccurred 2015-06-03 16:37:15,781 ERROR [Dispatcher thread: Central] recovery.RecoveryService: Recovery failure occurred. Skipping all events 2015-06-03 16:37:15,781 INFO [HistoryEventHandlingThread] impl.SimpleHistoryLoggingService: Writing event VERTEX_FINISHED to history file {noformat}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2533">TEZ-2533</a> | <em>Major</em> | <strong>AM deadlock when shutdown</strong></li>
</ul>
<p>AM is shutdown due to AM_REBOOT signal {noformat} 2015-06-03 15:44:25,637 INFO [Dispatcher thread: Central] app.DAGAppMaster: Received an AM_REBOOT signal 2015-06-03 15:44:25,637 INFO [Dispatcher thread: Central] app.DAGAppMaster: DAGAppMasterShutdownHandler invoked 2015-06-03 15:44:25,637 INFO [Dispatcher thread: Central] app.DAGAppMaster: Handling DAGAppMaster shutdown 2015-06-03 15:44:25,639 INFO [AMShutdownThread] app.DAGAppMaster: Calling stop for all the services 2015-06-03 15:44:25,640 INFO [AMShutdownThread] history.HistoryEventHandler: Stopping HistoryEventHandler 2015-06-03 15:44:25,640 INFO [AMShutdownThread] recovery.RecoveryService: Stopping RecoveryService 2015-06-03 15:44:25,640 INFO [AMShutdownThread] recovery.RecoveryService: Handle the remaining events in queue, queue size=0 2015-06-03 15:44:25,640 INFO [RecoveryEventHandlingThread] recovery.RecoveryService: EventQueue take interrupted. Returning {noformat} Then AM is failed to shutdown {noformat} 2015-06-03 15:44:25,814 WARN [AMShutdownThread] app.DAGAppMaster: Graceful stop failed {noformat} And at the same time AM shutdown hook is invoked and hang there, wait for lock of DAGAppMaster#shutdownHandlerRunning {noformat} 2015-06-03 15:44:25,818 INFO [Thread-1] app.DAGAppMaster: DAGAppMasterShutdownHook invoked 2015-06-03 15:44:25,818 INFO [Thread-1] app.DAGAppMaster: The shutdown handler is still running, waiting for it to complete {noformat}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2523">TEZ-2523</a> | <em>Major</em> | <strong>Tez UI: derive applicationId from dag/vertex id instead of relying on json data</strong></li>
</ul>
<p>currently the applicationId for the models dag/vertex is picked up from primary filter data for the dag/vertex json. deriving this from the dagid/vertexid on the models has the below benefits. * ensures applicationId is not null (in some corner cases this causes exception in store.find) * makes the ui backward compatible (0.5). * allows to remove the appid from primary filter (TEZ-2485)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2513">TEZ-2513</a> | <em>Major</em> | <strong>Tez UI: Allow filtering by DAG ID on All dags table</strong></li>
</ul>
<p>On using the filter display the record with the specific id, if not available display a No Records found message.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2511">TEZ-2511</a> | <em>Major</em> | <strong>Add exitCode to diagnostics when container fails</strong></li>
</ul>
<p>Currently we only identify the container failure cause for PREEMPTED / DISKS_FAILED, that means except for these 2 cases, there would be no clear diagnostic message for other cases.</p>
<p>{code} String message = &quot;Container completed. &quot;; TaskAttemptTerminationCause errCause = TaskAttemptTerminationCause.CONTAINER_EXITED; int exitStatus = containerStatus.getExitStatus(); if (exitStatus == ContainerExitStatus.PREEMPTED) { message = &quot;Container preempted externally. &quot;; errCause = TaskAttemptTerminationCause.EXTERNAL_PREEMPTION; } else if (exitStatus == ContainerExitStatus.DISKS_FAILED) { message = &quot;Container disk failed. &quot;; errCause = TaskAttemptTerminationCause.NODE_DISK_ERROR; } else if (exitStatus != ContainerExitStatus.SUCCESS){ message = &quot;Container failed. &quot;; } if (containerStatus.getDiagnostics() != null) { message += containerStatus.getDiagnostics(); } sendEvent(new AMContainerEventCompleted(amContainer.getContainerId(), exitStatus, message, errCause)); {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2509">TEZ-2509</a> | <em>Major</em> | <strong>YarnTaskSchedulerService should not try to allocate containers if AM is shutting down</strong></li>
</ul>
<p>Observed when doing some recovery testing:</p>
<p>Failure as during dag shutdown, 4 attempts of the same task failed.</p>
<p>{code} 2015-06-01 07:38:27,184 INFO [Dispatcher thread: Central] history.HistoryEventHandler: [HISTORY][DAG:dag_1433141118424_0012_2][Event:TASK_FINISHED]: vertexName=initialmap, taskId=task_1433141118424_0012_2_00_000003, startTime=1433144297281, finishTime=1433144307184, timeTaken=9903, status=FAILED, successfulAttemptID=null, diagnostics=TaskAttempt 0 failed, info=[Container container_e02_1433141118424_0012_01_000018 hit an invalid transition - C_NM_STOP_SENT at RUNNING] TaskAttempt 1 failed, info=[AttemptId: attempt_1433141118424_0012_2_00_000003_1 cannot be allocated to container: container_e02_1433141118424_0012_01_000011 in STOP_REQUESTED state] TaskAttempt 2 failed, info=[Container container_e02_1433141118424_0012_01_000012 hit an invalid transition - C_NM_STOP_SENT at RUNNING] TaskAttempt 3 failed, info=[Container container_e02_1433141118424_0012_01_000025 hit an invalid transition - C_NM_STOP_SENT at RUNNING], counters=Counters: 0 {code}</p>
<p>DAG kill signal received. {code} 2015-06-01 07:38:25,811 INFO [Thread-3] app.DAGAppMaster: DAGAppMasterShutdownHook invoked 2015-06-01 07:38:25,811 INFO [Thread-3] app.DAGAppMaster: DAGAppMaster received a signal. Signaling TaskScheduler {code}</p>
<p>First attempt marked as failed as container was killed. {code} 2015-06-01 07:38:26,906 INFO [Dispatcher thread: Central] history.HistoryEventHandler: [HISTORY][DAG:dag_1433141118424_0012_2][Event:TASK_ATTEMPT_FINISHED]: vertexName=initialmap, taskAttemptId=attempt_1433141118424_0012_2_00_000003_0, startTime=1433144297281, finishTime=1433144306904, timeTaken=9623, status=FAILED, errorEnum=FRAMEWORK_ERROR, diagnostics=Container container_e02_1433141118424_0012_01_000018 hit an invalid transition - C_NM_STOP_SENT at RUNNING, counters=Counters: 0 {code}</p>
<p>Subsequent attempt scheduled, assigned and eventually fails. {code} 2015-06-01 07:38:26,919 INFO [DelayedContainerManager] rm.YarnTaskSchedulerService: Assigning container to task, container=Container: [ContainerId: container_e02_1433141118424_0012_01_000011, NodeId: ip-172-31-18-41.ec2.internal:45454, NodeHttpAddress: ip-172-31-18-41.ec2.internal:8042, Resource: <memory:1536, vCores:1>, Priority: 2, Token: Token { kind: ContainerToken, service: 172.31.18.41:45454 }, ], task=attempt_1433141118424_0012_2_00_000003_1, containerHost=ip-172-31, localityMatchType=NodeLocal, matchedLocation=ip-172-31-18-41.ec2.internal, honorLocalityFlags=true, reusedContainer=true, delayedContainers=4, containerResourceMemory=1536, containerResourceVCores=1 {code}</p>
<p>Scheduler stops too late. {code} 2015-06-01 07:38:27,403 DEBUG [Thread-3] service.AbstractService: Service: org.apache.tez.dag.app.rm.YarnTaskSchedulerService entered state STOPPED {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2506">TEZ-2506</a> | <em>Major</em> | <strong>TestAysncHttpConnection failing</strong></li>
</ul>
<p>{code} https://builds.apache.org/job/PreCommit-TEZ-Build/767//testReport/org.apache.tez.http/TestHttpConnection/testAsyncHttpConnectionInterrupt/ {code}</p>
<p>{code} 2015-05-29 20:19:01,802 INFO [Thread-0] netty.AsyncHttpConnection (AsyncHttpConnection.java:initClient(78)) - Initializing AsyncClient (TezBodyDeferringAsyncHandler) 2015-05-29 20:19:02,057 ERROR [pool-4-thread-1] netty.TezBodyDeferringAsyncHandler (TezBodyDeferringAsyncHandler.java:onThrowable(84)) - Error in asyncHandler java.net.ConnectException: http://10.255.255.255:10221/ at com.ning.http.client.providers.netty.NettyConnectListener.operationComplete(NettyConnectListener.java:104) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:145) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.doConnect(NettyAsyncHttpProvider.java:1139) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.execute(NettyAsyncHttpProvider.java:944) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.nextRequest(NettyAsyncHttpProvider.java:1404) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.nextRequest(NettyAsyncHttpProvider.java:1400) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.remotelyClosed(NettyAsyncHttpProvider.java:1521) at com.ning.http.client.providers.netty.NettyConnectListener.operationComplete(NettyConnectListener.java:96) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:145) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.doConnect(NettyAsyncHttpProvider.java:1139) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.execute(NettyAsyncHttpProvider.java:940) at com.ning.http.client.AsyncHttpClient.executeRequest(AsyncHttpClient.java:499) at org.apache.tez.http.async.netty.AsyncHttpConnection.connect(AsyncHttpConnection.java:154) at org.apache.tez.http.async.netty.AsyncHttpConnection$$EnhancerByMockitoWithCGLIB$$93c3f8e5.CGLIB$connect$3(<generated>) at org.apache.tez.http.async.netty.AsyncHttpConnection$$EnhancerByMockitoWithCGLIB$$93c3f8e5$$FastClassByMockitoWithCGLIB$$b69a0361.invoke(<generated>) at org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216) at org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10) at org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22) at org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27) at org.mockito.internal.invocation.InvocationImpl.callRealMethod(InvocationImpl.java:112) at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36) at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:93) at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:38) at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:61) at org.apache.tez.http.async.netty.AsyncHttpConnection$$EnhancerByMockitoWithCGLIB$$93c3f8e5.connect(<generated>) at org.apache.tez.http.TestHttpConnection$Worker.call(TestHttpConnection.java:185) at org.apache.tez.http.TestHttpConnection$Worker.call(TestHttpConnection.java:170) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Caused by: java.nio.channels.ClosedByInterruptException at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:681) at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108) at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70) at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54) at org.jboss.netty.handler.codec.http.HttpClientCodec.handleDownstream(HttpClientCodec.java:97) at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleDownstream(ChunkedWriteHandler.java:109) at org.jboss.netty.channel.Channels.connect(Channels.java:634) at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207) at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229) at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182) at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.doConnect(NettyAsyncHttpProvider.java:1100) ... 30 more {code}</p>
<p>[~rajesh.balamohan] - could you pleas take a look.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2503">TEZ-2503</a> | <em>Minor</em> | <strong>findbugs version isn't reported properly in test-patch report</strong></li>
</ul>
<p>Post TEZ-1883</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2496">TEZ-2496</a> | <em>Major</em> | <strong>Consider scheduling tasks in ShuffleVertexManager based on the partition sizes from the source</strong></li>
</ul>
<p>Consider scheduling tasks in ShuffleVertexManager based on the partition sizes from the source. This would be helpful in scenarios, where there is limited resources (or concurrent jobs running or multiple waves) with dataskew and the task which gets large amount of data gets sceheduled much later.</p>
<p>e.g Consider the following hive query running in a queue with limited capacity (42 slots in total) @ 200 GB scale</p>
<p>{noformat} CREATE TEMPORARY TABLE sampleData AS SELECT CASE WHEN ss_sold_time_sk IS NULL THEN 70429 ELSE ss_sold_time_sk END AS ss_sold_time_sk, ss_item_sk, ss_customer_sk, ss_cdemo_sk, ss_hdemo_sk, ss_addr_sk, ss_store_sk, ss_promo_sk, ss_ticket_number, ss_quantity, ss_wholesale_cost, ss_list_price, ss_sales_price, ss_ext_discount_amt, ss_ext_sales_price, ss_ext_wholesale_cost, ss_ext_list_price, ss_ext_tax, ss_coupon_amt, ss_net_paid, ss_net_paid_inc_tax, ss_net_profit, ss_sold_date_sk FROM store_sales distribute by ss_sold_time_sk; {noformat}</p>
<p>This generated 39 maps and 134 reduce slots (3 reduce waves). When lots of nulls are there for ss_sold_time_sk, it would tend to have data skew towards 70429. If the reducer which gets this data gets scheduled much earlier (i.e in first wave itself), entire job would finish fast.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2489">TEZ-2489</a> | <em>Major</em> | <strong>Disable warn log for Timeline ACL error when tez.allow.disabled.timeline-domains set to true</strong></li>
</ul>
<p>15/05/26 22:57:38 WARN client.TezClient: Could not instantiate object for org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager. ACLs cannot be enforced correctly for history data in Timeline org.apache.tez.dag.api.TezUncheckedException: Unable to load class: org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager at org.apache.tez.common.ReflectionUtils.getClazz(ReflectionUtils.java:45) at org.apache.tez.common.ReflectionUtils.createClazzInstance(ReflectionUtils.java:88) at org.apache.tez.client.TezClient.start(TezClient.java:317) at cascading.flow.tez.planner.Hadoop2TezFlowStepJob.internalNonBlockingStart(Hadoop2TezFlowStepJob.java:137) at cascading.flow.planner.FlowStepJob.blockOnJob(FlowStepJob.java:248) at cascading.flow.planner.FlowStepJob.start(FlowStepJob.java:172) at cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:134) at cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:45) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.ClassNotFoundException: org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355)</p>
<p>Reported by @chris wensel</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2482">TEZ-2482</a> | <em>Major</em> | <strong>Tez UI: Mouse events not working on IE11</strong></li>
</ul>
<p>In IE 11 mouse events are not delivered to the page anymore at all after a SVG use element which has was under the mouse is removed - https://connect.microsoft.com/IE/feedback/details/796745</p>
<p>this affects IE on win7 and win2012 r2</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2481">TEZ-2481</a> | <em>Major</em> | <strong>Tez UI: graphical view does not render properly on IE11</strong></li>
</ul>
<p>The issue was because of IE's poor/broken support of css in SVG.</p>
<h1 id="ie-doesnt-support-transform-in-css-like-other-browsers.-this-caused-the-bubbles-in-a-vertex-to-appear-at-the-origin---httpsconnect.microsoft.comiefeedbackdetailview920928">IE doesn't support transform in css like other browsers. This caused the bubbles in a vertex to appear at the origin - https://connect.microsoft.com/IE/feedbackdetail/view/920928</h1>
<h1 id="ie-have-a-broken-support-for-the-markerarrow-on-the-path.-this-was-causing-the-linkspaths-to-disappear---httpsconnect.microsoft.comiefeedbackdetails801938">IE have a broken support for the marker(Arrow on the path). This was causing the links/paths to disappear - https://connect.microsoft.com/IE/feedback/details/801938</h1>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2475">TEZ-2475</a> | <em>Major</em> | <strong>Tez local mode hanging in big testsuite</strong></li>
</ul>
<p>we have a big test suite for lingual, our SQL layer for cascading. We are trying very hard to make it work correctly on Tez, but I am stuck:</p>
<p>The setup is a huge suite of SQL based tests (6000+), which are being executed in order in local mode. At certain moments the whole process just stops. Nothing gets executed any longer. This is not all the time, but quite often. Note that it is not happening at the same line of code, more at random, which makes it quite complex to debug.</p>
<p>What I am seeing, is these kind of stacktraces in the middle of the run:</p>
<p>2015-05-21 16:07:42,413 ERROR [TaskHeartbeatThread] task.TezTaskRunner (TezTaskRunner.java:reportError(333)) - TaskReporter reported error java.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2188) at org.apache.tez.runtime.task.TaskReporter$HeartbeatCallable.call(TaskReporter.java:187) at org.apache.tez.runtime.task.TaskReporter$HeartbeatCallable.call(TaskReporter.java:118) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)</p>
<p>This looks like it could be related to the hang, but the hang is not happening immediately afterwards, but some time later.</p>
<p>I have gone through quite a few JIRAs and saw that there were problems with locks and hanging threads before, which should be fixed, but it still happens.</p>
<p>I have tried 0.6.1 and 0.7.0. Both show the same behaviour.</p>
<p>This gist contains a thread dump of a hanging build: https://gist.github.com/fs111/1ee44469bf5cc31e5a52</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2473">TEZ-2473</a> | <em>Major</em> | <strong>Consider using RawLocalFileSystem in MapOutput.createDiskMapOutput</strong></li>
</ul>
<p>Currently it makes use of LocalFileSystem which would go through checksumming. This can save some CPU cycles in tasks involving disk merges.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2468">TEZ-2468</a> | <em>Major</em> | <strong>Change master to build against Java 7</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this incompatible change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2461">TEZ-2461</a> | <em>Major</em> | <strong>tez-history-parser compile fails with hadoop-2.4</strong></li>
</ul>
<p>https://builds.apache.org/job/Tez-Build-Hadoop-2.4/98/console</p>
<p></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2455">TEZ-2455</a> | <em>Major</em> | <strong>Tez UI: Dag view caching, error handling and minor layout changes</strong></li>
</ul>
<h1 id="enable-caching-in-dag-view.">Enable caching in Dag View.</h1>
<h1 id="view-throws-error-when-all-vertex-data-are-not-loaded">View throws error when all vertex data are not loaded</h1>
<h1 id="display-single-outputs-directly-below-the-vertex">Display single outputs directly below the vertex</h1>
<h1 id="vetex-input-nodes-at-the-same-level-slightly-overlaps-at-times">Vetex &amp; Input nodes at the same level slightly overlaps at times</h1>
<h1 id="entities-under-a-dag-not-loading-in-ie">Entities under a DAG not loading in IE</h1>
<h1 id="prevent-all-dags-page-from-hitting-ats-with-a-huge-limit-when-rowcount-is-manually-edited">Prevent 'All DAGs' page from hitting ATS with a huge limit when rowCount is manually edited</h1>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2454">TEZ-2454</a> | <em>Major</em> | <strong>Change FetcherOrderedGroup to work as Callables instead of blocking threads</strong></li>
</ul>
<p>The Fetcher threads for Ordered Input currently run and block till merge completes, which makes it difficult to use them via ThreadPools.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2453">TEZ-2453</a> | <em>Major</em> | <strong>Tez UI: show the dagInfo is the application has set the same.</strong></li>
</ul>
<p>hive/pig etc can set additional info on the dag using</p>
<p>{code} /** * Set description info for this DAG that can be used for visualization purposes. * @param dagInfo JSON blob as a serialized string. * Recognized keys by the UI are: * &quot;context&quot; - The application context in which this DAG is being used. * For example, this could be set to &quot;Hive&quot; or &quot;Pig&quot; if * this is being run as part of a Hive or Pig script. * &quot;description&quot; - General description on what this DAG is going to do. * In the case of Hive, this could be the SQL query text. * @return {@link DAG} */ {code} It would be useful to show this information.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2450">TEZ-2450</a> | <em>Major</em> | <strong>support async http clients in ordered &amp; unordered inputs</strong></li>
</ul>
<p>It will be helpful to switch between JDK &amp; other async http impls. For LLAP scenarios, it would be useful to make http clients interruptible which is supported in async libraries.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2447">TEZ-2447</a> | <em>Major</em> | <strong>Tez UI: Generic changes based on feedbacks.</strong></li>
</ul>
<ol>
<li>Status icon in all DAGs table is not inline with the text always.</li>
<li>Downloaded zip file must have type application/zip</li>
<li>KILLED status must be removed from All Dags status dropdown.</li>
<li>Text color must be made darker.</li>
</ol>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2446">TEZ-2446</a> | <em>Major</em> | <strong>Tez UI: Add tezVersion details when downloading timeline data for offline use</strong></li>
</ul>
<p>TEZ-2159 - Enables downloading timeline data for offline use. It would be good to add TEZ_APPLICATION (Which contains tezVersion, configs etc) along with this. </p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2440">TEZ-2440</a> | <em>Major</em> | <strong>Sorter should check for indexCacheList.size() in flush()</strong></li>
</ul>
<p>{noformat} 015-05-11 20:28:20,225 INFO [main] task.TezTaskRunner: Shutdown requested... returning 2015-05-11 20:28:20,225 INFO [main] task.TezChild: Got a shouldDie notification via hearbeats. Shutting down 2015-05-11 20:28:20,231 INFO [TezChild] impl.PipelinedSorter: Thread interrupted, cleaned up stale data, sorter threads shutdown=true, terminated=false 2015-05-11 20:28:20,231 INFO [TezChild] runtime.LogicalIOProcessorRuntimeTask: Joining on EventRouter 2015-05-11 20:28:20,231 INFO [TezChild] runtime.LogicalIOProcessorRuntimeTask: Ignoring interrupt while waiting for the router thread to die 2015-05-11 20:28:20,232 INFO [TezChild] task.TezTaskRunner: Encounted an error while executing task: attempt_1429683757595_0875_1_07_000000_0 java.lang.ArrayIndexOutOfBoundsException: -1 at java.util.ArrayList.elementData(ArrayList.java:418) at java.util.ArrayList.get(ArrayList.java:431) at org.apache.tez.runtime.library.common.sort.impl.PipelinedSorter.flush(PipelinedSorter.java:462) at org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput.close(OrderedPartitionedKVOutput.java:183) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.close(LogicalIOProcessorRuntimeTask.java:360) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) {noformat}</p>
<p>When a DAG is killed in the middle, sometimes these exceptions are thrown (e.g q_17 in TPC-DS). Even though it is completely harmless, it would be better to fix it to avoid distraction when debugging</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2436">TEZ-2436</a> | <em>Major</em> | <strong>Tez UI: Add cancel button in column selector.</strong></li>
</ul>
<p>Add cancel button beside OK and realign select all checkbox.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2423">TEZ-2423</a> | <em>Major</em> | <strong>Tez UI: Remove Attempt Index column from task-&gt;attempts page</strong></li>
</ul>
<p>Attempt Index and Attempt No serves the same purpose.</p>
<hr />
<ul>
<li><p><a href="https://issues.apache.org/jira/browse/TEZ-2406">TEZ-2406</a> | <em>Major</em> | <strong>Tez UI: Display per-io counter columns in task and attempt pages under vertex</strong></p></li>
<li>We will auto-populate all the counter names including io counter names to the tasks (under a vertex) and task attempts (under task, vertex).</li>
<li>To enable navigation the column names will be searchable in the pop-up for column selection.</li>
<li><p>Per-io counter names will not be stored in the personalization settings given they are dag / vertex specific.</p></li>
</ul>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2391">TEZ-2391</a> | <em>Blocker</em> | <strong>TestVertexImpl timing out at times on jenkins builds</strong></li>
</ul>
<p>For example, https://builds.apache.org/job/Tez-Build/1028/console</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2378">TEZ-2378</a> | <em>Major</em> | <strong>In case Fetcher (unordered) fails to do local fetch, log in debug mode to reduce log size</strong></li>
</ul>
<p>Following can be logged as debug mode as opposed to WARN level. May be counters can be added later to track the number of times it failed to do local-fetch.</p>
<p>{noformat} 2015-04-28 05:41:45,487 WARN [Fetcher [Map_5] #15] shuffle.Fetcher: Failed to shuffle output of InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=81], attemptNumber=0, pathComponent=attempt_1429683757595_0485_1_03_000081_0_10003, fetchTypeInfo=FINAL_MERGE_ENABLED, spillEventId=-1] from cn047-10.l42scl.hortonworks.com(local fetch) org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find output/attempt_1429683757595_0485_1_03_000081_0_10003/file.out.index in any of the configured local directories at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:449) at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164) at org.apache.tez.runtime.library.common.shuffle.Fetcher.getShuffleInputFileName(Fetcher.java:612) at org.apache.tez.runtime.library.common.shuffle.Fetcher.getTezIndexRecord(Fetcher.java:592) at org.apache.tez.runtime.library.common.shuffle.Fetcher.doLocalDiskFetch(Fetcher.java:537) at org.apache.tez.runtime.library.common.shuffle.Fetcher.doSharedFetch(Fetcher.java:353) at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:192) at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:72) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) {noformat}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2376">TEZ-2376</a> | <em>Major</em> | <strong>Remove TaskAttemptEventType.TA_DIAGNOSTICS_UPDATE</strong></li>
</ul>
<p>It is never used.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2311">TEZ-2311</a> | <em>Major</em> | <strong>AM can hang if kill received while recovering from previous attempt</strong></li>
</ul>
<p>We saw an instance of a Tez job hanging despite receiving multiple kill requests from clients. The AM was recovering from a prior attempt when the first kill request arrived.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2291">TEZ-2291</a> | <em>Major</em> | <strong>TEZ UI: Improper vertex name in tables.</strong></li>
</ul>
<p>-1. Counters values are not getting displayed in tables.- 2. There is probably a race condition where the vertex name does not get displayed - reproduction - go to all tasks page of one dag, go back to all dags, go to another dag all tasks, check if the vertex name is getting displayed. at times i was seeing vertex id instead. a refresh would show the vertex name.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2198">TEZ-2198</a> | <em>Major</em> | <strong>Fix sorter spill counts</strong></li>
</ul>
<p>Prior to pipelined shuffle, tez merged all spilled data into a single file. This ended up creating one index file and one output file. In this context, TaskCounter.ADDITIONAL_SPILL_COUNT was referred as the number of additional spills and there was no counter needed to track the number of merges.</p>
<p>With pipelined shuffle, there is no final merge and ADDITIONAL_SPILL_COUNT would be misleading, as these spills are direct output files which are consumed by the consumers.</p>
<p>It would be good to have the following - ADDITIONAL_SPILL_COUNT: represents the spills that are needed by the task to generate the final merged output - TOTAL_SPILLS: represents the total number of shuffle directories (index + output files) that got created at the end of processing.</p>
<p>For e.g, Assume sorter generated 5 spills in an attempt Without pipelining: ============== ADDITIONAL_SPILL_COUNT = 5 &lt;-- Additional spills involved in sorting TOTAL_SPILLS = 1 &lt;-- Final merged output</p>
<h1 id="with-pipelining">With pipelining:</h1>
<p>ADDITIONAL_SPILL_COUNT = 0 &lt;-- Additional spills involved in sorting TOTAL_SPILLS = 5 &lt;--- all spills are final output</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2172">TEZ-2172</a> | <em>Major</em> | <strong>FetcherOrderedGrouped using List to store InputAttemptIdentifier can lead to some inefficiency during remove() operation</strong></li>
</ul>
<p>As part of fixing TEZ-2001, FetcherOrderedGrouped stores InputAttemptIdentifier in List. This can lead to some inefficiency - since the size of this list can be ~30, and remove() calls can be expensive.</p>
<p>Option 1: by using the spillId in the hashCode - or a wrapping structure for just this. However, SpillId can not be added to the hashCode as it would break ShuffleScheduler shuffleInfoEventsMap.</p>
<p>Option 2: consider using Map with an identifier.</p>
<p>Need to consider other options as well. Creating this jira as a placeholder to fix this issue.</p>
<hr />
<ul>
<li><p><a href="https://issues.apache.org/jira/browse/TEZ-2076">TEZ-2076</a> | <em>Major</em> | <strong>Tez framework to extract/analyze data stored in ATS for specific dag</strong></p></li>
<li>Users should be able to download ATS data pertaining to a DAG from Tez-UI (more like a zip file containing DAG/Vertex/Task/TaskAttempt info).</li>
<li>This can be plugged to an analyzer which parses the data, adds semantics and provides an in-memory representation for further analysis.</li>
<li>This will enable to write different analyzer rules, which can be run on top of this in-memory representation to come up with analysis on the DAG.</li>
<li><p>Results of this analyzer rules can be rendered on to UI (standalone webapp) later point in time.</p></li>
</ul>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-2048">TEZ-2048</a> | <em>Blocker</em> | <strong>Remove VertexManagerPluginContext.getTaskContainer()</strong></li>
</ul>
<p>This should have been removed earlier. It exposes internal execution details that may not be safe to provide to user code.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-1970">TEZ-1970</a> | <em>Major</em> | <strong>Fix javadoc warnings in SortMergeJoinExample</strong></li>
</ul>
<p>test-patch reports 3 existing javadoc warnings.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-1961">TEZ-1961</a> | <em>Critical</em> | <strong>Remove misleading exception &quot;No running dag&quot; from AM logs</strong></li>
</ul>
<p>{code} 15/01/14 16:45:06 INFO ipc.Server: IPC Server handler 0 on 51000, call org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from Call#0 Retry#0 org.apache.tez.dag.api.TezException: No running dag at present at org.apache.tez.dag.api.client.DAGClientHandler.getDAG(DAGClientHandler.java:84) at org.apache.tez.dag.api.client.DAGClientHandler.getACLManager(DAGClientHandler.java:151) at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPBServerImpl.getDAGStatus(DAGClientAMProtocolBlockingPBServerImpl.java:94) at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolRPC$DAGClientAMProtocol$2.callBlockingMethod(DAGClientAMProtocolRPC.java:7375) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2041) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2037) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2035) 15/01/14 16:45:06 INFO client.DAGClientImpl: DAG initialized: CurrentState=Running {code}</p>
<p>This exception shows up fairly often and isn't very relevant - queries before a DAG is submitted to the AM. This is very misleading, especially for folks new to Tez, and should be removed.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-1752">TEZ-1752</a> | <em>Major</em> | <strong>Inputs / Outputs in the Runtime library should be interruptable</strong></li>
</ul>
<p>Not possible to preempt tasks without killing containers without this.</p>
<p>There's still the problem of Processors not supporting interrupts. We may need API enhancements to either query IPOs on whether they are interrupbtible.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-1529">TEZ-1529</a> | <em>Blocker</em> | <strong>ATS and TezClient integration in secure kerberos enabled cluster</strong></li>
</ul>
<p>This is a follow up for TEZ-1495 which address ATS - TezClient integration. however it does not enable it in secure kerberos enabled cluster.</p>
